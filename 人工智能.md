[TOC]

# 机器学习

## 支持向量机

**Support Vector Machine(SVM)**

<img src="assets/image-20250802155658886.png" alt="image-20250802155658886" style="zoom:80%;" />  

<img src="assets\image-20250410185711881.png" alt="image-20250410185711881" style="zoom:67%;" /> 

## 决策树

### 算法

<img src="assets/image-20250721123248643.png" alt="image-20250721123248643" style="zoom:50%;" /> 

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回: 

(1) 当前结点包含的样本全属于同一类别，无需划分; 

(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; 

(3) 当前结点包含的样本集合为空，不能划分.

在第(2)种情形下，我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;在第 (3) 种情形下，同样把当前结点标记为叶结点，将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不同：情形(2) 是在利用当前结点的后验分布，而情形(3) 则是把父结点的样本分布作为当前结点的先验分布. 

### 信息增益

<img src="assets/image-20250726002013120.png" alt="image-20250726002013120" style="zoom:80%;" /> 

### 基尼指数

<img src="assets/image-20250726002224034.png" alt="image-20250726002224034" style="zoom:80%;" /> 

### 剪枝  

 

## 集成学习

### Boosting

<img src="assets/image-20250808165307808.png" alt="image-20250808165307808" style="zoom:50%;" /> 

### Bagging

<img src="assets/image-20250808165415957.png" alt="image-20250808165415957" style="zoom:50%;" /> 

### 随机森林

**Random Forest (RF)**

- 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择.

- 传统决策树在 $d$ 个属性中选择一个最优属性进行划分，RF 先选择一个包含 $k$ 个属性的子集，再选择子集中的最优属性。

   $k=d$ 时，与传统决策树相同；$k=1$ 时，随机选择一个属性划分；一般推荐 $k=\log_2d$

## 精度

<img src="assets\image-20250423170702645.png" alt="image-20250423170702645" style="zoom:67%;" /> 

<img src="assets\image-20250423170753653.png" alt="image-20250423170753653" style="zoom:67%;" />  

<img src="assets\image-20250430200312137.png" alt="image-20250430200312137" style="zoom:67%;" /> 

<img src="assets\image-20250430200426708.png" alt="image-20250430200426708" style="zoom:67%;" /> 

<img src="assets\image-20250423171453105.png" alt="image-20250423171453105" style="zoom:67%;" />  

## 正则化（Regularization）

<img src="assets\image-20250410190811998.png" alt="image-20250410190811998" style="zoom:80%;" /> 

## 归一化（Normalization）

### RMS Norm

**Root mean square Norm**

![image-20250701162050277](assets/image-20250701162050277.png)

![image-20250701162326945](assets/image-20250701162326945.png) 

### Batch Norm

- 在推理时的均值方差需要固定，用的是训练时所有均值方差的 EMA

## 激活函数

### Sigmoid

<img src="assets\image-20250512194102742.png" alt="image-20250512194102742" style="zoom:67%;" /> 

### Tanh

<img src="assets\image-20250512194505385.png" alt="image-20250512194505385" style="zoom:67%;" />  

### Swish

<img src="assets/image-20250720174318662.png" alt="image-20250720174318662" style="zoom:50%;" /> 

<img src="assets/image-20250720174451060.png" alt="image-20250720174451060" style="zoom:80%;" /> 

### GELU

![image-20250701171959179](assets/image-20250701171959179.png) 

<img src="assets/image-20250701171728437.png" alt="image-20250701171728437" style="zoom:50%;" /> 

### SwiGLU

<img src="assets/image-20250701174055352.png" alt="image-20250701174055352" style="zoom:50%;" /> 

<img src="assets/image-20250701174325050.png" alt="image-20250701174325050" style="zoom: 50%;" /> 

## 优化器

### SGD

### Adam

## 损失函数

### Cross Entropy

$$
L(\hat{y})=-\sum{y\log{\hat{y}}}
$$

- $-\log$ 操作把概率空间转换到信息空间，真实 label 的预测概率越接近 1，蕴含的不确定信息越少，损失越接近 0。

### InfoNCE

$$
L_q=-\log{\frac{\exp(q\cdot k_+/\tau)}{\sum_{i=1}^{N} \exp(q\cdot k_i)/\tau}}
$$

- $q$ 是已经编码好的特征 query，$k_1,k_2...k_N$ 是已经编码好的样本，其中 $k_+$ 是和 $q$ 匹配的正样本，其他是负样本。

- $\tau$ 是温度系数，当 $\tau = 1$ 时，InfoNCE 就是交叉熵损失。

  当温度系数变大，$qk/\tau$ 减小，logits 分布变平滑，模型对负样本一视同仁，容易导致区分力不足

  当温度系数变小，$qk/\tau$ 增大，logits 分布更尖锐，模型更关注相似度高的样本。

- 通常用于对比学习，例如 CLIP, MoCo。

## 聚类

### 外部指标

将聚类结果与某个"参考模型"进行比较

<img src="assets/image-20250723174048923.png" alt="image-20250723174048923" style="zoom:50%;" /> 

<img src="assets/image-20250723174107834.png" alt="image-20250723174107834" style="zoom:50%;" /> 

<img src="assets/image-20250723174121450.png" alt="image-20250723174121450" style="zoom:50%;" /> 

### 内部指标

直接考察聚类结果而不利用任何参考模型。

<img src="assets/image-20250723174223241.png" alt="image-20250723174223241" style="zoom:50%;" /> 

<img src="assets/image-20250723174236951.png" alt="image-20250723174236951" style="zoom:50%;" /> 

### k 均值算法

<img src="assets/image-20250723174330631.png" alt="image-20250723174330631" style="zoom:50%;" /> 

<img src="assets/image-20250723174459643.png" alt="image-20250723174459643" style="zoom: 50%;" /> 

### 学习向量量化(LVQ)

**Learning Vector Quantization**

<img src="assets/image-20250723174656398.png" alt="image-20250723174656398" style="zoom:50%;" /> 

### DBSCAN

**Density-Based Spatial Clustering of Applications with Noise**

<img src="assets/image-20250723175037118.png" alt="image-20250723175037118" style="zoom:50%;" /> 

<img src="assets/image-20250723175058471.png" alt="image-20250723175058471" style="zoom:50%;" /> 

<img src="assets/image-20250723175113966.png" alt="image-20250723175113966" style="zoom:50%;" /> 

## 概率图模型

<img src="assets\image-20250512011127763.png" alt="image-20250512011127763" style="zoom:80%;" /> 

<img src="assets\image-20250512011543407.png" alt="image-20250512011543407" style="zoom:80%;" /> 

<img src="assets\image-20250512011556497.png" alt="image-20250512011556497" style="zoom:80%;" /> 

### HHM

### CRF

## PCA降维

<img src="assets\image-20250512100347297.png" alt="image-20250512100347297" style="zoom:57%;" /> 

<img src="assets\image-20250512101043525.png" alt="image-20250512101043525" style="zoom: 55%;" /> 

<img src="assets\image-20250512101639950.png" alt="image-20250512101639950" style="zoom: 67%;" />   

<img src="assets\image-20250512094732647.png" alt="image-20250512094732647" style="zoom: 67%;" /> 

<img src="assets\image-20250512095515538.png" alt="image-20250512095515538" style="zoom:67%;" /> 



# 深度学习

## 自编码器

<img src="assets\image-20250423004223975.png" alt="image-20250423004223975" style="zoom:67%;" /> 



<img src="https://pic4.zhimg.com/v2-4cb07847643ff8b6e4dd612b68472d95_1440w.jpg" alt="img" style="zoom: 80%;" /> 

<img src="assets\image-20250423103034998.png" alt="image-20250423103034998" style="zoom:67%;" /> 

### 重参数化(reparameterization)

<img src="assets\image-20250423014516357.png" alt="image-20250423014516357" style="zoom:67%;" /> 

<img src="assets\image-20250423014556711.png" alt="image-20250423014556711" style="zoom:67%;" /> 

```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        # 编码器部分
        self.fc1 = nn.Linear(input_dim, 512)
        self.fc21 = nn.Linear(512, latent_dim)  # 均值
        # 由于方差不能为负，所以这里输出的是log(方差)，而不是方差本身.
        self.fc22 = nn.Linear(512, latent_dim)  # 对数方差
 
        # 解码器部分
        self.fc3 = nn.Linear(latent_dim, 512)
        self.fc4 = nn.Linear(512, input_dim)
 
    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)
 
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # 从标准正态分布N(0, 1)中采样
        return mu + eps * std  # 重参数化公式
 
    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))  # 使用sigmoid激活函数输出到[0, 1]区间
 
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

## 图神经网络GNN

<img src="assets\image-20250430191924314.png" alt="image-20250430191924314" style="zoom:67%;" /> 

### GCN

<img src="assets\image-20250430193404994.png" alt="image-20250430193404994" style="zoom:67%;" /> 

![image-20250430194802463](assets\image-20250430194802463.png) 

<img src="assets\image-20250430194211056.png" alt="image-20250430194211056" style="zoom:67%;" /> 

![image-20250430195237020](assets\image-20250430195237020.png) 

<img src="assets\image-20250430193316692.png" alt="image-20250430193316692" style="zoom: 80%;" />



## 扩散模型

### DDPM

[[2006.11239] Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)

<img src="assets/image-20250802160006446.png" alt="image-20250802160006446" style="zoom:80%;" />  

**前向传播：**

- 前向传播可以视为马尔可夫过程，$t$ 时刻的图像 $x_t$ 只和 $x_{t-1}$ 有关。

  $q(x_{1:T}|x_0)=\prod_{t=1}^{T}q(x_{t}|x_{t-1})$，$q(x_{1:T}|x_0)$ 表示从 $x_0$ 开始添加 $T$ 次噪声后，图像的概率分布。

- $q(x_t|x_{t-1}) = N(x_{t-1};\sqrt{1-\beta_t}\cdot x_{t-1},\beta_tI)$

  建模每一步加噪的条件概率分布，是一个 $H\cdot W\cdot C$ 维度的高斯分布，均值是 $\sqrt{1-\beta_t}\cdot x_{t-1}$，协方差矩阵是 $\beta_tI$，每个时间步的值 $x_t$ 采样自这个高斯分布。

  $I$ 是一个 $HWC\times HWC$ 维度的单位矩阵，表示各维度独立，每个维度的方差都是 $\beta_t$。

- $\beta_t$ 用于控制每个时间步向数据中添加噪声的量。不同时间下的 $\beta_t$ 是预先定义好的，随时间递增，可以是Linear，Cosine等。满足 $\beta_1<\beta_2<...<\beta_T$。

  在扩散过程的早期，我们希望对数据添加较少的噪声，从而最大程度保留数据的原始结构；而在扩散过程的后期，我们则希望通过添加更多的噪声将数据逐渐转化为一个接近高斯分布的形式。

- 定义 $\alpha_t = 1-\beta_t$，$\overline{\alpha_t}=\prod_{i=1}^{t}\alpha_i$，推导得到

   $q(x_t|x_0)=N(x_t;\sqrt{\overline{\alpha_t}}\cdot x_0,(1-\sqrt{\overline{\alpha_t}})I)$.

<img src="assets/image-20250802160230070.png" alt="image-20250802160230070" style="zoom:80%;" /> 

**训练过程：**

- 从数据中抽取一个样本 $x_0$
- 随机选取一个时间步 $t\in [1,T]$
- 随机采样 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$
- 计算 $x_0$ 加噪后的图像 $x_t=\sqrt{\overline{\alpha_t}}\cdot x_0+\sqrt{1-\overline{\alpha_t}}\cdot \epsilon$
- 通过训练好的 Unet 模型 $\epsilon_\theta$，预测在时间步 $t$ 对应的噪声 $\epsilon_\theta(x_t, t)$
- 计算 Unet 预测噪声和随机噪声的 L2 损失，计算梯度，更新模型

### DDIM

[[2010.02502] Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)



### Stable Diffusion

[[2112.10752] High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

<img src="assets/image-20250802205229150.png" alt="image-20250802205229150" style="zoom:80%;" /> 

**创新：**

- 加入 AutoEncoder，使得扩散过程在 latent space 下执行，提高图像生成的效率。
- 加入条件控制机制，能够使用其他模态的数据控制图像的生成，其中条件生成控制通过 Cross Attention 机制实现。

**三大组件： VAE、U-Net、CLIP Text Encoder **

### DiT

**Diffusion Transformer**

[[2212.09748] Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748)

<img src="assets/image-20250803021632560.png" alt="image-20250803021632560" style="zoom: 50%;" /> 

### SDXL

[[2307.01952] SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952)

<img src="assets/image-20250803022201045.png" alt="image-20250803022201045" style="zoom: 80%;" /> 

- Stable Diffusion XL 是一个**二阶段的级联扩散模型**，包括 Base 模型和Refiner模型。
- Base模型的主要工作和Stable Diffusion 1.x-2.x一致，具备文生图、图生图、图像inpainting 等能力。
- 在Base模型之后，级联了Refiner模型，也是一个扩散模型，**对Base模型生成的图像Latent特征进行精细化提升，其本质上是在做图生图的工作**。

 

### SD3

## 卷积神经网络

**Convolutional Neural Network(CNN)**

### 深度可分离卷积（DSC）

<img src="assets\image-20250512172809025.png" alt="image-20250512172809025" style="zoom:67%;" /> 

<img src="assets\image-20250512173606480.png" alt="image-20250512173606480" style="zoom:80%;" /> 

<img src="assets\image-20250512173945141.png" alt="image-20250512173945141" style="zoom:67%;" /> 

<img src="assets\image-20250512174016883.png" alt="image-20250512174016883" style="zoom:67%;" /> 

# Transformer架构

## Transformer

<img src="assets\image-20250514022040178.png" alt="image-20250514022040178" style="zoom:67%;" /> 



## BERT

**Bidirectional Encoder Representation from Transformers**

<img src="assets/image-20250701175312162.png" alt="image-20250701175312162" style="zoom:67%;" /> 

Embedding由三种Embedding求和而成：

Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的

<img src="assets/image-20250701185220489.png" alt="image-20250701185220489" style="zoom: 67%;" /> 

<img src="assets/image-20250701202711001.png" alt="image-20250701202711001" style="zoom:67%;" /> 

![image-20250701202740359](assets/image-20250701202740359.png)

```python
class BERTLM(nn.Module):
    """
    BERT Language Model
    Next Sentence Prediction Model + Masked Language Model
    """

    def __init__(self, bert: BERT, vocab_size):
        """
        :param bert: BERT model which should be trained
        :param vocab_size: total vocab size for masked_lm
        """

        super().__init__()
        self.bert = bert
        self.next_sentence = NextSentencePrediction(self.bert.hidden)
        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)

    def forward(self, x, segment_label):
        x = self.bert(x, segment_label)
        return self.next_sentence(x), self.mask_lm(x)


class NextSentencePrediction(nn.Module):
    """
    2-class classification model : is_next, is_not_next
    """

    def __init__(self, hidden):
        """
        :param hidden: BERT model output size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, 2)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x[:, 0]))


class MaskedLanguageModel(nn.Module):
    """
    predicting origin token from masked input sequence
    n-class classification problem, n-class = vocab_size
    """

    def __init__(self, hidden, vocab_size):
        """
        :param hidden: output size of BERT model
        :param vocab_size: total vocab size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x))
```

## GPT

## Attention

### Self Attention

<img src="assets\image-20250516205208397.png" alt="image-20250516205208397" style="zoom:67%;" />  



### Cross Attention

与 Self-Attention 不同，Self-Attention 是让一个序列自己内部的元素相互关注（比如一个句子中的单词互相计算关系），而 Cross Attention 则是让**两个不同的序列**之间建立关注关系。

Cross Attention 的核心在于：它允许一个序列（称为 Query，查询）去关注另一个序列（称为 Key 和 Value，键和值），从而实现信息的融合。

<img src="assets\image-20250516205343160.png" alt="image-20250516205343160" style="zoom:67%;" /> 

<img src="assets\image-20250516205834217.png" alt="image-20250516205834217" style="zoom:67%;" /> 



### MQA

**(Multi-Query Attention)**

MQA 让所有的头之间 **共享** 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。

我们知道了 MQA 实际上是将 head 中的 key 和 value 矩阵抽出来单独存为一份共享参数，而 query 则是依旧保留在原来的 head 中，每个 head 有一份自己独有的 query 参数。

### GQA

**(Grouped-Query Attention)**

GQA将查询头分成G组，对于Query是每个头单独保留一份参数，每个组共享一个Key 和 Value 矩阵。GQA-G 是指具有 G 组的 grouped-query attention。

中间组数导致插值模型的质量高于 MQA，但比 MHA 更快。从 MHA 到 MQA 将 H 键和值头减少到单个键和值头，减少了键值缓存的大小，因此需要加载的数据量 H 倍。

<img src="assets\image-20250517014338254.png" alt="image-20250517014338254" style="zoom:67%;" /> 

### MLA

**(Multi-Head Latent Attention)**

### FlashAttention

[Flash Attention论文解读 - 李理的博客](https://fancyerii.github.io/2023/10/23/flashattention/)

<img src="assets\image-20250520020712996.png" alt="image-20250520020712996" style="zoom:67%;" /> 

**核心思想**：传统减少HBM的访问，将QKV切分为小块后放入SRAM中

**核心方法**：tiling, recomputation 

FlashAttention旨在**加速**注意力计算并**减少内存占用**。FlashAttention利用底层硬件的内存层次知识，例如GPU的内存层次结构，来提高计算速度和减少内存访问开销。 FlashAttention的核心原理是通过将输入**分块**并在每个块上执行注意力操作，从而减少对高带宽内存（HBM）的读写操作。

具体而言，FlashAttention 使用平铺和重计算等经典技术，将输入块从HBM加载到 SRAM（快速缓存），在SRAM上执行注意力操作，并将结果更新回HBM。FlashAttention减少了内存读写量，从而实现了**2-4倍** 的时钟时间加速。

<img src="assets\image-20250520015144621.png" alt="image-20250520015144621" style="zoom:67%;" /> 

<img src="assets\image-20250520015118296.png" alt="image-20250520015118296" style="zoom:67%;" /> 

<img src="assets\image-20250520020743919.png" alt="image-20250520020743919" style="zoom: 67%;" /> 

## Swin Transformer

<img src="assets/image-20250704204931292.png" alt="image-20250704204931292" style="zoom:80%;" /> 

 <img src="assets/image-20250704205134536.png" alt="image-20250704205134536" style="zoom:67%;" /> 

 ## Vision Transformer

[[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

<img src="assets/image-20250706161732340.png" alt="image-20250706161732340" style="zoom:80%;" /> 

**模型架构：**

- 将输入图片分成若干个 patch (例如 16*16 ) , 然后将每个 patch **经过线性层投影**为固定长度的向量作为一个 token。
- 进行图片分类任务时，在输入序列中加入一个特殊的 token [class]，该 token 的输出则为最后的预测结果。

<img src="assets/image-20250808180110559.png" alt="image-20250808180110559" style="zoom:67%;" /> 

**变体：**

- ViT-H（High resolution）：ViT-H是Vision Transformer模型中的高分辨率变体。它通常适用于处理高分辨率图像或更具挑战性的视觉任务。由于处理高分辨率图像可能需要更多的计算资源和内存，因此ViT-H模型可能更庞大和复杂。
- ViT-L（Low resolution）：ViT-L是Vision Transformer模型中的低分辨率变体。它通常用于处理低分辨率图像或资源受限的环境。ViT-L模型可能比ViT-H模型更小、更轻量级，适合在资源受限的设备或场景中部署。
- ViT-B（Base resolution）：ViT-B是Vision Transformer模型中的基准分辨率变体。它可以被视为ViT模型的中间规模。ViT-B通常是指在资源充足但不需要处理过高或过低分辨率图像时使用的模型。

## 位置编码

### RoPE

https://arxiv.org/abs/2104.09864

<img src="assets/image-20250715164636630.png" alt="image-20250715164636630" style="zoom:67%;" />

<img src="assets/image-20250715164457895.png" alt="image-20250715164457895" style="zoom:67%;" /> 

``` python
def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):
    # 计算词向量元素两两分组之后，每组元素对应的旋转角度
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]
    t = torch.arange(seq_len, device=freqs.device)
    # freqs.shape = [seq_len, dim // 2] 
    freqs = torch.outer(t, freqs).float()
    # torch.polar 的文档
    # https://pytorch.org/docs/stable/generated/torch.polar.html
    # 计算结果是个复数向量
    # 假设 freqs = [x, y]
    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    # xq.shape = [batch_size, seq_len, dim]
    # xq_.shape = [batch_size, seq_len, dim // 2, 2]
    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)
    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)
    
    # 转为复数域
    xq_ = torch.view_as_complex(xq_)
    xk_ = torch.view_as_complex(xk_)
    
    # 应用旋转操作，然后将结果转回实数域
    # xq_out.shape = [batch_size, seq_len, dim]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.wq = Linear(...)
        self.wk = Linear(...)
        self.wv = Linear(...)
        
        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)

    def forward(self, x: torch.Tensor):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(batch_size, seq_len, dim)
        xk = xk.view(batch_size, seq_len, dim)
        xv = xv.view(batch_size, seq_len, dim)

        # attention 操作之前，应用旋转位置编码
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # scores.shape = (bs, seqlen, seqlen)
        scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim)
        scores = F.softmax(scores.float(), dim=-1)
        output = torch.matmul(scores, xv)  # (batch_size, seq_len, dim)
  # ......
```

# 强化学习

强化学习（Reinforcement learning，RL）讨论的问题是一个**智能体(agent)** 怎么在一个复杂不确定的 **环境(environment)** 里面去极大化它能获得的奖励。通过感知所处环境的 **状态(state)** 对 **动作(action)** 的 **反应(reward)**， 来指导更好的动作，从而获得最大的 **收益(return)**，这被称为在交互中学习，这样的学习方法就被称作强化学习。

<img src="assets/image-20250810143047096.png" alt="image-20250810143047096" style="zoom: 67%;" /> 

 **特征：** 

- **试错学习**：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。
- **延迟回报**：强化学习的指导信息很少，得到的奖励是延迟的，往往在事后（最后一个状态）才给出。
- 训练过程中得到的数据是有时间关联的数据（**sequential data**），而不是独立同分布的数据。
- 智能体的动作会影响随后得到的数据。在训练智能体的过程中，很多时候我们通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。

**基本元素：**

<img src="assets/image-20250810152620357.png" alt="image-20250810152620357" style="zoom:67%;" /> 

- **环境(Environment)** 是一个外部系统，智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。
- **智能体(Agent)** 是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态。
- **状态(State)/观察值(Observation)**：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。
- **动作(Action)**：不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces)，例如，走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间;如果机器人向 360◦ 中的任意角度都可以移动，则为连续动作空间。
- **奖励(Reward)**：是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了智能体在某一步采取了某个策略的表现如何。

**相关定义：**

- **回报(Return)**

  - 又称累计未来奖励**(cumulated future reward)**，一般表示为 $G$。

    $G_t=\sum\limits_{k=0}^{\infty}R_{t+k}=R_t+R_{t+1}+R_{t+2}+...$

  - 由于未来的奖励不如即时的等值奖励，所以 $R_{t+1}$ 的权重应该小于 $R_t$，通常使用**累积折扣奖励（Discounted Cumulative Reward）**

    引入折扣因子 $\gamma \in [0,1]$，有 $G_t=\sum\limits_{k=0}^{\infty}\gamma^kR_{t+k}=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...$ 

- **状态价值函数(State-value Function)**

  - 用来度量给定策略 $\pi$ 的情况下，当前状态 $s_t$ 的好坏程度，表示基于 $t$ 时刻的状态 $s$ 能获得的未来回报（return）的期望。

    $v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]$

- **动作价值函数(Action-value Function)**

  - 用来度量给定状态 $s_t$ 和策略 $\pi$ 的情况下，采用动作 $a_t$ 的好坏程度，表示基于 $t$ 时刻的状态 $s$，选择动作 $a$ 后能获得的未来回报（return）的期望。

    $q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=t,A_t=a]$

- **轨迹(Trajectory)**

  - 轨迹就是当前帧和它采取的一系列策略，即状态和动作的序列

    $\tau=(s_0,a_0,s_1,a_1,...)$

**算法分类：**

**按照环境是否已知划分：免模型学习（Model-Free） vs 有模型学习（Model-Based）**

- **Model-free**：不去学习和理解环境，环境给出什么信息就是什么信息，常见的方法有 policy optimization 和 Q-learning。
- **Model-Based**：学习和理解环境，学会用一个模型来模拟环境，通过模拟的环境来得到反馈。Model-Based 相当于比Model-Free多了模拟环境这个环节，通过模拟环境预判接下来会发生的所有情况，然后选择最佳的情况。

> 一般情况下，环境都是不可知的，所以这里主要研究无模型问题。

**按照学习方式划分：在线策略（On-Policy） vs 离线策略（Off-Policy）**

- **On-Policy**：agent 必须本人在场， 并且一定是本人边玩边学习。典型的算法为 Sarsa。
- **Off-Policy**：agent 可以选择自己玩， 也可以选择看着别人玩， 通过看别人玩来学习别人的行为准则， 离线学习同样是从过往的经验中学习， 但是这些过往的经历没必要是自己的经历， 任何人的经历都能被学习，也没有必要是边玩边学习，玩和学习的时间可以不同步。典型的方法是Q-learning，以及Deep-Q-Network。

**按照学习目标划分：基于策略（Policy-Based）和基于价值（Value-Based）。**

- **Policy-Based**：直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。
- **Value-Based**：输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa。
- **Actor-Critic**：二者的结合，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程，常见的有A2C，A3C，DDPG等。

<img src="assets/image-20250810153053305.png" alt="image-20250810153053305" style="zoom:67%;" /> 

## Q-Learning

-  基于表格，没有神经网络参与，当状态和动作空间是离散且维数不高时使用。
- 维护一张 **Q 值表**，表的维数为：状态数S * 动作数A，每个数代表在状态 S 下采用动作 A 可以获得的未来收益的折现和。通过不断迭代 Q 值表使其最终收敛，然后根据 Q 值表在每个状态下选取一个最优策略。

**步骤：**

1. 初始化一张全 0 的 Q 值表。

2. 通过 Epsilon-Greedy 策略选择一个动作并执行。

   - 将策略的选择方式分为两种：**探索(Exploration)**和利用**(Exploitation)**，探索即随机选择一个动作，利用即选择 Q 值表中值最大的动作。
   - 以 $\epsilon$ 的概率进行探索，以 $1-\epsilon$ 的概率进行利用。$\epsilon$ 一般初始为 1，随训练时间而减小。

3. 通过得到的奖励 $R(s,a)$ 来更新 Q 值表：

   $新估计值 \leftarrow 旧估计值+学习率 \times [目标-旧估计值]$

   $Q(s,a) \leftarrow Q(s,a)+\alpha(R(s,a)+\gamma \max\limits_{a'}{R(s',a')}-Q(s,a))$

## 策略梯度

## DQN

## PPO

**proxomal policy optimization，近端策略优化**

## DPO

**Direct Preference Optimization，直接偏好优化**

## RLHF

**Reinforcement Learning from Human Feedback，人类反馈强化学习**

<img src="assets/image-20250701205352001.png" alt="image-20250701205352001" style="zoom:50%;" /> 

 

# 计算机视觉(CV)

## 特征处理

### 梯度直方图

**Histogram of oriental gradient(HOG)**

### DPM

**Deformable part model(DPM)**：可变形的组件模型

## 超分辨率Super-Resolution

<img src="assets\image-20250403230441211.png" alt="image-20250403230441211" style="zoom:80%;" /> 

### 插值

<img src="assets\image-20250403223119126.png" alt="image-20250403223119126" style="zoom:67%;" /> 

<img src="assets\image-20250403221621550.png" alt="image-20250403221621550" style="zoom: 67%;" /> 

<img src="assets\image-20250403221651797.png" alt="image-20250403221651797" style="zoom:80%;" /> 

<img src="assets\image-20250403222350685.png" alt="image-20250403222350685" style="zoom:67%;" /> 

<img src="assets\image-20250403222407214.png" alt="image-20250403222407214" style="zoom:67%;" /> 

<img src="assets\image-20250403222500027.png" alt="image-20250403222500027" style="zoom:67%;" /> 

<img src="assets\image-20250403222851881.png" alt="image-20250403222851881" style="zoom:67%;" />  

<img src="assets\image-20250403230104523.png" alt="image-20250403230104523" style="zoom:67%;" /> 

<img src="assets\image-20250403230120013.png" alt="image-20250403230120013" style="zoom:80%;" />  

<img src="assets\image-20250403223101220.png" alt="image-20250403223101220" style="zoom:67%;" /> 

<img src="assets\image-20250403225647531.png" alt="image-20250403225647531" style="zoom:67%;" /> 

## 图像分割

### 语义分割

### 实例分割

### SAM

[[2304.02643] Segment Anything](https://arxiv.org/abs/2304.02643)

<img src="assets/image-20250706150623193.png" alt="image-20250706150623193" style="zoom:80%;" /> 

<img src="assets/image-20250706152441484.png" alt="image-20250706152441484" style="zoom:80%;" /> 

### SEEM 

[[2304.06718] Segment Everything Everywhere All at Once](https://arxiv.org/abs/2304.06718)

<img src="assets/image-20250706153823730.png" alt="image-20250706153823730" style="zoom:80%;" /> 

<img src="assets/image-20250706153916422.png" alt="image-20250706153916422" style="zoom:80%;" /> 

  

## 目标检测

### yolo

### SSD

### faster RCNN

### DINO

[[2203.03605] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection](https://arxiv.org/abs/2203.03605)



<img src="assets/image-20250706181139516.png" alt="image-20250706181139516" style="zoom:80%;" /> 

 

# 自然语言处理(NLP)

## TF-IDF

<img src="assets\image-20250517175934886.png" alt="image-20250517175934886" style="zoom:80%;" /> 

<img src="assets\image-20250517180016963.png" alt="image-20250517180016963" style="zoom:67%;" /> 

 <img src="assets\image-20250517180244359.png" alt="image-20250517180244359" style="zoom:67%;" /> 

<img src="assets\image-20250517182015899.png" alt="image-20250517182015899" style="zoom:67%;" /> 

 <img src="assets\image-20250517182311893.png" alt="image-20250517182311893" style="zoom:67%;" /> 

TF−IWF 这种加权方法 **降低了文档集/语料库中同类文本对词权重的影响，更加精确地表达了词在待查文档中的重要程度**。

传统 TF−IDF 所求的权值一般很小，甚至接近于 0，精确度也不高，而 TF−IWF 的计算结果恰能解决权值过小的问题。

## LDA 主题模型

**Latent Dirichlet Allocation(LDA)**

- 潜在狄利克雷分配模型，是一种**文档生成模型**，也是**无监督学习**模型（需要手动输入主题数量）。

- LDA 模型认为：每个文档是由多个主题混合而成的，而每个主题是由多个词语混合而成的。

  混合的意思是：每个主题/词语都按照一定的概率分布出现。

- LDA 模型把文档的构造看作一个概率过程：首先以一定的概率选择某个主题，然后在这个主题下以一定的概率选择某个词语。不断重复这个过程从而生成整篇文章。

- LDA 模型是一种词袋模型，假设词与词之间不存在顺序关系。优点：简化算法。 

<img src="assets\image-20250518021410497.png" alt="image-20250518021410497" style="zoom:80%;" /> 

## n-gram

- 它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。

  每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。

- 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的 Bi-Gram 和三元的 Tri-Gram。

## Tokenizer

 <img src="assets\image-20250519001025901.png" alt="image-20250519001025901" style="zoom:67%;" /> 

<img src="assets\image-20250519001402351.png" alt="image-20250519001402351" style="zoom: 67%;" /> 

<img src="assets\image-20250519001448704.png" alt="image-20250519001448704" style="zoom:67%;" /> 

<img src="assets\image-20250519001841783.png" alt="image-20250519001841783" style="zoom:67%;" /> 

<img src="assets\image-20250519005132956.png" alt="image-20250519005132956" style="zoom:67%;" /> 

 <img src="assets\image-20250519005146975.png" alt="image-20250519005146975" style="zoom: 80%;" /> 

<img src="assets\image-20250519003305498.png" alt="image-20250519003305498" style="zoom:67%;" /> 

### BPE

<img src="assets\image-20250519002401003.png" alt="image-20250519002401003" style="zoom:67%;" /> 

<img src="assets\image-20250519005314906.png" alt="image-20250519005314906" style="zoom:80%;" />  

<img src="assets\image-20250519004346279.png" alt="image-20250519004346279" style="zoom: 80%;" /> 

<img src="assets\image-20250519004607890.png" alt="image-20250519004607890" style="zoom:80%;" /> 

### WordPiece

<img src="assets\image-20250519005951093.png" alt="image-20250519005951093" style="zoom:80%;" /> 

### Unigram

<img src="assets\image-20250519010829154.png" alt="image-20250519010829154" style="zoom:80%;" /> 

<img src="assets\image-20250519010742662.png" alt="image-20250519010742662" style="zoom:80%;" /> 

<img src="assets\image-20250519011215732.png" alt="image-20250519011215732" style="zoom:80%;" /> 

# 分布式训练

## 数据并行(DP/DDP)

<img src="assets\image-20250430134619762.png" alt="image-20250430134619762" style="zoom:67%;" /> 

<img src="assets\image-20250430135137120.png" alt="image-20250430135137120" style="zoom:67%;" /> 

<img src="assets\image-20250430180050311.png" alt="image-20250430180050311" style="zoom:67%;" /> 

<img src="assets\image-20250430180835259.png" alt="image-20250430180835259" style="zoom:80%;" /> 

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms

# 定义模型（示例用简单的线性模型）
class MyModel(torch.nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = torch.nn.Linear(10, 10)
    def forward(self, x):
        return self.fc(x)

# 训练函数
def train(rank, world_size):
    # 初始化进程组
    # 常见的通信后端包括 gloo 和 nccl，其中 nccl 是在 GPU 上最常用的后端
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)

    # 设置当前 GPU
    torch.cuda.set_device(rank)  # 若是在多机上，此处设置为local_rank

    # 创建模型并转移到当前 GPU
    model = MyModel().cuda(rank)  # 若是在多机上，此处设置为local_rank

    # 将模型包装为 DDP 模型
    model = DDP(model, device_ids=[rank])	# 若是在多机上，此处设置为local_rank

    # 定义损失函数和优化器
    criterion = torch.nn.CrossEntropyLoss().cuda(rank)	# 若是在多机上，此处设置为local_rank
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    # 加载数据集（示例使用 CIFAR10）
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

    # 使用 DistributedSampler 确保每个 GPU 得到不同的数据
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)	# 若是在多机上，此处设置为local_rank
    train_loader = DataLoader(dataset=train_dataset, batch_size=32, sampler=train_sampler)

    # 开始训练
    for epoch in range(10):
        model.train()
        train_sampler.set_epoch(epoch)  # 每个 epoch 设置随机种子以保证不同进程之间的数据一致
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.cuda(rank), target.cuda(rank)	# 若是在多机上，此处设置为local_rank
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # 销毁进程组
    dist.destroy_process_group()

# 主函数
def main():
    world_size = 2  # 使用 2 张 GPU

    # 使用 multiprocessing 启动多进程，每个 GPU 对应一个进程
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

<img src="assets\image-20250430180610671.png" alt="image-20250430180610671" style="zoom: 80%;" /> 

<img src="assets\image-20250430180641620.png" alt="image-20250430180641620" style="zoom:80%;" /> 

## 流水线并行(PP)

**Pipeline Parallelism**

### GPipe

https://arxiv.org/pdf/1811.06965

<img src="assets/image-20250801175312885.png" alt="image-20250801175312885" style="zoom:80%;" />

## 张量并行(TP)



## DeepSpeed

# 推荐系统

<img src="assets\image-20250517192945429.png" alt="image-20250517192945429" style="zoom:67%;" /> 

<img src="assets\image-20250517193115047.png" alt="image-20250517193115047" style="zoom: 80%;" /> 

<img src="assets\image-20250517193147276.png" alt="image-20250517193147276" style="zoom: 80%;" /> 

## 召回

<img src="assets\image-20250517193240590.png" alt="image-20250517193240590" style="zoom:80%;" /> 

# 模型轻量化

## 蒸馏

<img src="assets\image-20250423153200155.png" alt="image-20250423153200155" style="zoom:67%;" /> 

<img src="assets\image-20250423153133783.png" alt="image-20250423153133783" style="zoom:80%;" /> 

<img src="assets\image-20250423153745152.png" alt="image-20250423153745152" style="zoom:67%;" /> 

<img src="assets\image-20250423153759589.png" alt="image-20250423153759589" style="zoom:67%;" /> 

<img src="assets\image-20250423153301493.png" alt="image-20250423153301493" style="zoom: 67%;" /> 

<img src="assets\image-20250423153634116.png" alt="image-20250423153634116" style="zoom: 67%;" /> 

## 量化

<img src="assets\image-20250512221958977.png" alt="image-20250512221958977" style="zoom: 50%;" /> 

<img src="assets\image-20250512223845596.png" alt="image-20250512223845596" style="zoom:80%;" /> 

<img src="assets\image-20250512223751019.png" alt="image-20250512223751019" style="zoom: 67%;" /> 

<img src="assets\image-20250512223812038.png" alt="image-20250512223812038" style="zoom:67%;" /> 

<img src="assets\image-20250512223822901.png" alt="image-20250512223822901" style="zoom: 67%;" /> 

###  训练后量化(PTQ)

**Post-training-quantization**

<img src="assets\image-20250512224531232.png" alt="image-20250512224531232" style="zoom: 67%;" /> 

<img src="assets\image-20250512224557276.png" alt="image-20250512224557276" style="zoom: 67%;" /> 

### 量化感知训练(QAT)

**Quantization-aware-training**

<img src="assets\image-20250514012604076.png" alt="image-20250514012604076" style="zoom:67%;" /> 

## 低秩分解

# 大语言模型

## 微调

### 低秩适应(LoRA)

**Low-Rank Adaptation**

<img src="assets\image-20250514020113800.png" alt="image-20250514020113800" style="zoom:80%;" /> 

<img src="assets\image-20250514021140044.png" alt="image-20250514021140044" style="zoom: 67%;" /> 

<img src="assets\image-20250514021009857.png" alt="image-20250514021009857" style="zoom:67%;" /> 

### 监督微调（SFT）

**Supervised Fine-Tuning**



## Tool Learning

<img src="assets\image-20250515103539089.png" alt="image-20250515103539089" style="zoom:67%;" /> 

<img src="assets\image-20250515103951754.png" alt="image-20250515103951754" style="zoom: 67%;" /> 

<img src="assets\image-20250515104113191.png" alt="image-20250515104113191" style="zoom: 67%;" /> 

<img src="assets\image-20250515102328271.png" alt="image-20250515102328271" style="zoom:67%;" /> 

<img src="assets\image-20250515104237189.png" alt="image-20250515104237189" style="zoom:67%;" /> 

 <img src="assets\image-20250515104344725.png" alt="image-20250515104344725" style="zoom:67%;" /> 



## 多模态

<img src="assets\image-20250407132547580.png" alt="image-20250407132547580" style="zoom: 67%;" />  

### 对比学习

- 是一种特殊的无监督学习方法（自监督学习），通过最大化相关样本之间的相似性并最小化不相关样本之间的相似性来学习数据表示。通常使用一种高自由度、自定义的规则生成正负样本。在模型预训练中有广泛的应用。 

<img src="assets\image-20250407125808900.png" alt="image-20250407125808900" style="zoom:80%;" /> 

<img src="assets\image-20250407125834294.png" alt="image-20250407125834294" style="zoom:67%;" /> 

<img src="assets\image-20250407130218548.png" alt="image-20250407130218548" style="zoom:67%;" /> 

<img src="assets\image-20250407130709955.png" alt="image-20250407130709955"  /> <img src="assets\image-20250407130721905.png" alt="image-20250407130721905" style="zoom:67%;" />  

<img src="assets\image-20250407130751914.png" alt="image-20250407130751914" style="zoom: 67%;" /> 

<img src="assets\image-20250407130853721.png" alt="image-20250407130853721" style="zoom:67%;" /> 

#### CLIP

**Contrastive Language-Image Pretraining**

[[2103.00020] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

- 是视觉-语言对齐的经典模型，使用图像文本双编码器，通过大规模的图文对进行对比学习。
- 对比学习：学习一个共享嵌入空间，使得正确的图文对特征距离最小化，错误的图文对特征距离最大化。
- 大型预训练：在大规模的多模态数据上进行预训练，学会基础的跨模态知识，然后通过微调来实现特定任务。
- 优点：架构简洁，效果好。在 **zero-shot 文本-图像检索，zero-shot 图像分类，open-domain 检测分割**等任务上效果非常好。

- 动机：将 NLP 中大型预训练的框架+对比学习迁移在多模态对齐上。

**训练目标：**

- 预测图像与文本配对，通过最大化图像与文本嵌入的余弦相似度（正样本）并最小化错误配对的相似度（负样本），优化对称交叉熵损失（InfoNCE 损失）。给定一批 N 个图像文本对，CLIP 学习预测 N×N 个可能配对中实际发生的配对，计算图像和文本嵌入的成对余弦相似度（经温度参数缩放），然后通过对称交叉熵损失优化。

<img src="assets\image-20250407121854146.png" alt="image-20250407121854146" style="zoom:67%;" /> 

![image-20250407121957171](assets\image-20250407121957171.png) 

**预训练**

- 模型的输入是若干个 图像-文本 对，假设每个 batch 有 N 个图像文本对。
- 图像：通过一个 Image Encoder 得到图像特征，encoder 是 ResNet 或者 **Vision Transformer**，得到 N 个图像特征 $(I_1,I_2...I_N)$
- 文本：通过一个 Text Encoder 得到文本特征，encoder 是 **Bert**，得到 $(T_1,T_2...T_N)$
- 构建正负样本进行对比学习。配对的图像文本对作为正样本（即对角线部分 $I_1-T_1,I_2-T_2...I_N-T_N$)，剩余图像文本对为负样本。
- 数据集：**WIT (WebImage Text)**，由 OpenAI 专门收集，有4亿个图像文本对，且质量高，数据清理较好。

**推理**

![image-20250407122205488](assets\image-20250407122205488.png)

<img src="assets\image-20250407122422622.png" alt="image-20250407122422622" style="zoom:80%;" />

<img src="assets\image-20250407123811712.png" alt="image-20250407123811712" style="zoom:67%;" /> 

#### CLAP

<img src="assets\image-20250502174823331.png" alt="image-20250502174823331" style="zoom:80%;" /> 

#### BLIP

https://arxiv.org/abs/2201.12086

<img src="assets/image-20250808180633241.png" alt="image-20250808180633241" style="zoom:50%;" /> 

**模型架构：**

- **image encoder**: 图像编码器，用于提取视觉特征，初始化为在ImageNet上预训练的ViT，该模型需要与其他3个模型联合训练，没有单独的训练目标。
- **text encoder**: 文本编码器，用于提取文本特征，初始化为Bert。该模型与image encoder联合训练的目标为 ITC（Image-Text Contrastive Loss），ITC 将图像和文本映射到相同的特征空间（与CLIP类似）。
- **image-grounded text encoder：**图像文本编码器，用于判断文本和图像是否匹配。该模型的输入为[Encode] +文本，另外每个block层都以交叉注意力的形式关注**image encoder**对应层的视觉特征。训练目标为ITM(Image-Text Matching Loss)，即对当前文本和图像是否匹配进行二分类。
- **image-grounded text decoder：**图像文本解码器，用于生成图像的caption。交叉注意力机制的引入与image-grounded text encoder类似，训练目标为 LM 损失(Language Modeling Loss)。

**优化目标：**

- **图像文本对比损失（ITC）**：通过引入动量编码器，鼓励正图像文本对在特征空间中具有相似表示，与负对形成对比，**从而对齐视觉和语言的特征空间**。具体计算时，将图像文本对的输出特征通过特定方式计算相似度，优化对比损失以最大化正样本相似度并最小化负样本相似度。
- **图像文本匹配损失（ITM）**：将其作为二分类任务，利用图像接地文本编码器学习图像文本的多模态表示，通过 ITM 头（线性层）预测图像文本对是否匹配，采用硬负样本挖掘策略选择更有信息量的负样本进行损失计算。
- **语言建模损失（LM）**：基于图像接地文本解码器，以自回归方式生成文本描述，优化交叉熵损失，在计算时应用标签平滑（0.1）以提高模型的泛化能力。

#### BLIP2

https://arxiv.org/abs/2301.12597

<img src="assets/image-20250808180850309.png" alt="image-20250808180850309" style="zoom:50%;" />  

![image-20250808180908640](assets/image-20250808180908640.png) 

### DINO

[[2104.14294] Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

<img src="assets/image-20250706183029340.png" alt="image-20250706183029340" style="zoom:80%;" /> 

### VLM

## MoE 架构

[A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

<img src="assets\image-20250505183854419.png" alt="image-20250505183854419" style="zoom: 50%;" /> <img src="assets\image-20250505183955344.png" alt="image-20250505183955344" style="zoom:50%;" />   

**核心思想：**

- 把传统 Transformer 模型里每个前馈网络 (FFN) 层替换成一个 MOE 层，每个 MOE 层由专家网络和门控网络组成。 

  每个专家在训练过程中学习不同的信息，而在推理时仅使用和当前任务最相关的特定专家。

- **专家网络(Experts)：**

  每个专家是一个独立的子网络（通常是 FFN），在实际计算中只有部分专家会被激活并参与处理。通过让多个专家分担不同数据子集的计算，模型在预训练时可以以较低的计算开销获得大参数量带来的表示能力。

- **门控网络(Gating/Router)：**

  负责根据输入 token 的特征动态选择激活哪些专家。门控网络一般采用一个带 softmax 的简单前馈网络来计算每个专家的权重。经过训练后，门控网络会逐步学会将相似的输入路由到表现更好的专家。 

<img src="assets\image-20250505182657503.png" alt="image-20250505182657503" style="zoom:67%;" /> 

**选择最适合该任务的专家：**

<img src="assets\image-20250505182947162.png" alt="image-20250505182947162" style="zoom:67%;" /> 

路由器（或门控网络）本身也是一个 FFN，它根据特定的输入选择专家。路由器会输出概率值，并利用这些概率来选择最匹配的专家：

**专家层返回被选定专家的输出，并乘以门控值（选择概率）。**

路由器和专家（其中仅选择少部分）共同构成了 MoE 层。

给定的 MoE 层有两种类型：稀疏专家混合模型（Sparse Mixture of Experts）和密集专家混合模型（Dense Mixture of Experts）。

两者都使用路由器来选择专家，但稀疏 MoE 只选择少数几个专家，而密集 MoE 则选择全部专家，但可能会以不同的分布进行选择。

<img src="assets\image-20250505184157768.png" alt="image-20250505184157768" style="zoom: 67%;" /> 

### 负载均衡

<img src="assets\image-20250505192124109.png" alt="image-20250505192124109" style="zoom:50%;" /> 

然而，这个简单的功能往往会导致路由器总是选择相同的专家，因为某些专家可能比其他专家学习得更快。

这不仅会导致专家选择的不均匀分布，还会导致某些专家几乎没有被训练过。这会在训练和推理过程中引发问题。

因此，我们希望在训练和推理期间，各个专家的使用具有同等的重要性，这就是所谓的负载平衡。某种程度上，这是为了防止模型在同一组专家上过拟合。

#### KeepTopK

一种对路由器进行负载平衡的方法是使用一个简单的扩展策略，称为 KeepTopK。通过引入可训练的（高斯）噪声，我们可以防止总是选择相同的专家：

<img src="assets\image-20250505185544253.png" alt="image-20250505185544253" style="zoom: 67%;" /> 

然后，除希望激活的前 k 个专家（例如 2 个）以外的所有专家权重都将被设为 $-\infty$。

将这些权重设为 $-\infty$ 时，SoftMax 操作后的输出概率将变为 0。

<img src="assets\image-20250505194542278.png" alt="image-20250505194542278" style="zoom: 67%;" /> 



#### 辅助损失

#### 专家容量

### Switch Transformer

### GShard

### Vision-MoE

### 优势

- **提高模型性能**：通过将多个专家的预测结果进行整合，MoE 模型可以在不同的数据子集或任务方面发挥每个专家的优势，从而提高整体模型的性能。例如，在图像分类任务中，一个专家可能擅长识别动物图片，而另一个专家可能擅长识别车辆图片，通过门控网络的合理分配，MoE 模型可以更准确地对不同类型的图片进行分类。
- **减少计算成本**：与传统的密集模型相比，MoE 模型在处理每个输入样本时，只有相关的专家会被激活，而不是整个模型的所有参数都被使用。这意味着 MoE 模型可以在保持较高性能的同时，显著减少计算资源的消耗，特别是在模型规模较大时，这种优势更为明显。例如，对于一个具有数十亿参数的大型语言模型，采用 MoE 架构可以在不增加太多计算成本的情况下，通过增加专家的数量来进一步提升模型的性能。
- **增强模型的可扩展性**：MoE 模型的架构设计使得它可以很容易地扩展到更多的专家和更大的模型规模。通过增加专家的数量，模型可以覆盖更广泛的数据特征和任务类型，从而在不增加计算复杂度的情况下，提升模型的表达能力和泛化能力。这种可扩展性为处理大规模、复杂的数据集提供了有效的解决方案，例如在处理多模态数据（包含文本、图像、语音等多种类型的数据）时，MoE 模型可以通过设置不同的专家来专门处理不同模态的数据，实现更高效的多模态融合。

### 挑战

- 计算成本与资源管理
  - **内存需求高**：MoE模型需要将所有专家的参数都加载到内存中，即使在推理过程中只使用其中一部分专家。例如，以Mixtral 8x7B这样的MoE模型为例，需要有足够的VRAM来容纳一个47B参数的稠密模型。这是因为MoE模型中只有FFN层被视为独立的专家，而模型的其他参数是共享的。高内存需求使得在资源有限的情况下部署和运行MoE模型变得困难，特别是在需要处理大规模参数模型时，对硬件设备的要求更为苛刻。
  - **分布式训练复杂**：为了应对大规模模型的训练，通常需要采用分布式训练的方式。但在MoE模型中，由于专家之间的数据交换和并行训练需要机间all-to-all通信来实现，这增加了通信成本。随着模型规模的增大，通信开销也相应增加，可能导致训练效率降低。例如，在一个大规模分布式训练场景中，若模型参数规模达到数十亿甚至更大，通信延迟和网络拥塞问题可能会严重影响训练速度和性能。因此，在实际部署过程中，需要仔细设计通信策略和优化网络拓扑，以降低通信延迟和潜在的网络拥塞。
  - **专家容量限制**：为了防止特定专家过载并确保工作负载平衡，通常会对每个专家可以同时处理的输入数量设置阈值。例如，采用top-2路由和1.25的容量因子，这意味着每个输入选择两个专家，每个专家处理其通常容量的1.25倍。这种策略虽然可以在一定程度上平衡负载，但也可能导致部分数据无法及时处理或需要重新分配，影响训练和推理的效率。此外，专家容量的设置需要根据具体的任务和模型规模进行调整，这增加了模型配置和管理的复杂性。
- 过拟合与泛化问题
  - **过拟合风险**：与稠密模型相比，MoE模型在微调时更易产生过拟合现象。这是因为MoE模型的参数量虽然大，但在实际应用中只激活部分专家，模型的复杂度相对较高。例如，拥有1.6T参数量的MoE预训练模型Switch Transformer，在SuperGLUE等常见基准上进行微调时，其整体性能却落后于较小的模型。这表明在微调过程中，模型可能会过度拟合训练数据中的噪声和细节，而无法很好地泛化到新的、未见过的数据上。
  - **泛化能力不足**：MoE模型的泛化能力在某些任务上表现不佳，尤其是在需要对输入数据进行深入理解和推理的任务中。例如，在重理解任务（如SuperGLUE）上，MoE模型的表现不如对应的稠密模型。这可能是因为MoE模型在训练过程中，专家之间的协作和知识共享不够充分，导致模型对特定任务的理解和处理能力有限。此外，门控网络的设计和训练也可能影响模型的泛化能力，如果门控网络不能准确地将输入数据分配给最合适的专家，就会影响模型的整体性能。
  - **微调策略选择**：为了提高MoE模型在微调阶段的泛化能力，需要选择合适的微调策略。一种可行的方法是尝试冻结所有非专家层的权重，只对MoE层的参数进行更新。实验结果显示，这种方法几乎与更新所有参数的效果相当，同时可以加速微调过程并降低显存需求。此外，使用较小的批量大小和较高的学习率进行微调，也有助于提高模型的泛化性能。然而，这些策略的选择需要根据具体的任务和模型情况进行调整，不同的任务可能需要不同的微调策略来达到最佳效果。

## RAG

**Retrieval-Augmented Generation，检索增强生成**

- 通过给大模型提供外部的知识库，根据用户提示检索出相关信息，合并到 prompt 中输入给大模型，以增强模型处理知识密集型任务的能力，例如问答、文本摘要、内容生成等。

**动机：**

- **知识的局限性**：模型的训练数据基本来自网络上的公开数据，缺少一些离线的、实时的、非公开的信息和知识。
- **幻觉**：大模型不具备某一方面的知识时或在不擅长的场景中，容易出现幻觉问题。通过提供对应领域的知识，可以减轻幻觉问题。
- **数据安全**：由于数据安全的考虑，企业不愿意把私域数据放在第三方平台训练。通过RAG提供数据库可以为大模型提供私有的数据。

**步骤：**

1. **准备知识文档**

   从多个数据源收集数据，并做好数据清理，将 csv、pdf 等文件清理成大语言模型可以理解的纯文本数据。

   如果文档过长，还需要将长篇文档分割成多个文本块，以便更高效地处理和检索信息。

2. **训练嵌入模型**

   训练 Word2Vec / BERT 等嵌入模型，将文本转换为向量表示，然后通过计算它们之间的相似度便可以确定它们的关联程度。

3. **构建向量数据库**

   向量数据库是专门设计用于存储和检索向量数据的数据库系统。在RAG系统中，通过嵌入模型生成的所有向量都会被存储在这样的数据库中，优化了处理和存储大规模向量数据的效率。

4. **查询检索**

   用户的问题会被输入到嵌入模型中进行向量化处理。然后，系统会在向量数据库中搜索与该问题向量语义上相似的知识文本或历史对话记录并返回。

5. **生成回答**

   将用户提问和检索到的信息结合，构建出一个提示模版，输入到大语言模型中生成回答。

**优化：**

- 多级索引：将庞大的数据和信息需求按类别划分，并在不同层级中组织，以实现更有效的管理和检索。
- 数据分块：把文档分割成多个文本块再进行向量嵌入，其目的是在保持语义上的连贯性的同时，尽可能减少嵌入内容中的噪声，从而更有效地找到与用户查询最相关的文档部分。如果分块太大，可能包含太多不相关的信息，从而降低了检索的准确性。相反，分块太小可能会丢失必要的上下文信息，导致生成的回应缺乏连贯性或深度。

## 开源大模型

### 千问系列

#### Qwen 2.5

对于稠密模型，团队延续了 Qwen2 的 Transformer 解码器架构，并在此基础上进行了优化。该架构包括以下关键组件：

- **分组查询注意力（Grouped Query Attention, GQA）**：用于高效地利用 KV 缓存。
- **SwiGLU 激活函数**：增强非线性激活。
- **旋转位置编码（RoPE）**：用于编码位置信息。
- **QKV 偏置**：用于提升注意力机制的表现。
- **RMSNorm**：在预归一化后使用，以保证训练过程稳定。
- **BBPE（Bytewise Byte Pair Encoding）**：用于分词。

#### Qwen 3

<img src="assets/image-20250704205903234.png" alt="image-20250704205903234" style="zoom:80%;" /> 

#### Qwen 2 vl

https://arxiv.org/abs/2409.12191

1. **读懂不同分辨率和不同长宽比的图片**：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
2. **理解20分钟以上的长视频**：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
3. **能够操作手机和机器人的视觉智能体**：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
4. **多语言支持**：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。

模型架构：整体上我们仍然延续了 Qwen-VL 中 ViT 加 Qwen2 的串联结构，在三个不同尺度的模型上，我们都采用 600M 规模大小的 ViT，并且支持图像和视频统一输入。为了让模型更清楚地感知视觉信息和理解视频，我们还进行了以下升级：

- 实现了对**原生动态分辨率**的全面支持。与上一代模型相比，Qwen2-VL 能够处理任意分辨率的图像输入，不同大小图片被转换为动态数量的 tokens，最小只占 4 个 tokens。

  这种设计不仅确保了模型输入与图像原始信息之间的高度一致性，更是模拟了人类视觉感知的自然方式，赋予模型处理任意尺寸图像的强大能力，使其在图像处理领域展现出更加灵活和高效的表现。

- **多模态旋转位置嵌入（M-ROPE）**。传统的旋转位置嵌入只能捕捉一维序列的位置信息，而 M-ROPE 通过将原始旋转嵌入分解为代表时间、高度和宽度的三个部分，使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息。

  这一创新赋予了语言模型强大的多模态处理和推理能力，能够更好地理解和建模复杂的多模态数据。

<img src="assets/image-20250716160806924.png" alt="image-20250716160806924" style="zoom: 50%;" /> 



#### Qwen 2.5 vl

https://arxiv.org/abs/2502.13923

<img src="assets/image-20250716115339642.png" alt="image-20250716115339642" style="zoom:50%;" /> 



### GPT系列

### DeepSeek系列

#### DeepSeek-v3

[[2412.19437] DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)

<img src="assets/image-20250727185032313.png" alt="image-20250727185032313" style="zoom: 80%;" /> 

#### DeekSeep-R1

[[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

- 我们直接将RL应用于基础模型，而不依赖监督微调 (SFT) 作为初步步骤。这种方法允许模型探索解决复杂问题的思维链 (CoT)，从而得到DeepSeek-R1-Zero。DeepSeek-R1-Zero展示了自我验证、反思和生成长思维链等能力，标志着研究界的一个重要里程碑。值得注意的是，这是首次公开研究，验证了LLMs的推理能力可以纯粹通过RL来激励，而不需要SFT。

### LLaMA系列

#### LLaMA

[[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- 只使用了公开数据集进行训练，完全可复现、可分享。
- 预归一化：使用 **RMSNorm**对 Transformer 层的输入进行归一化。
- **SwiGLU **激活函数。
- **RoPE** 位置编码。
- **AdamW** 优化器，**cosine 学习率调度**。

#### LLaMA-3

[The Llama 3 Herd of Models | Research - AI at Meta](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)

<img src="assets/image-20250727193954139.png" alt="image-20250727193954139" style="zoom:80%;" /> 

<img src="assets/image-20250727194138629.png" alt="image-20250727194138629" style="zoom:80%;" />  

###   LLaVA系列

#### LLaVA

[[2304.08485] Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

<img src="assets/image-20250706155302142.png" alt="image-20250706155302142" style="zoom:80%;" /> 

#### LLaVA-1.5

[[2310.03744] Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)

<img src="assets/image-20250706160511953.png" alt="image-20250706160511953" style="zoom:80%;" /> 

#### LLaVA-OneVision

[[2408.03326] LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)

<img src="assets/image-20250706160640853.png" alt="image-20250706160640853" style="zoom:80%;" /> 

 

## 推理分割（Reasoning Segmentation）

### LISA（CVPR2024）

[[2308.00692] LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/abs/2308.00692)

[(43 封私信 / 80 条消息) 论文阅读笔记：LISA: Reasoning Segmentation via Large Language Model - 知乎](https://zhuanlan.zhihu.com/p/656977939)

<img src="assets/image-20250706154427655.png" alt="image-20250706154427655" style="zoom:80%;" /> 

<img src="assets/image-20250706164643164.png" alt="image-20250706164643164" style="zoom:67%;" /><img src="assets/image-20250706145942268.png" alt="image-20250706145942268" style="zoom:80%;" /> 

#### 贡献点

- 我们介绍了推理分割任务，该任务需要基于隐式人类指令进行推理。这项任务强调了自我推理能力的重要性，这对于构建真正智能的感知系统至关重要。
- 我们建立了一个包含一千多个图像指令对的推理分割 Benchmark：ReasonSeg。对于评估和鼓励社区开发新技术至关重要。
- 我们提出了我们的模型：LISA，它采用嵌入式掩码范例来整合新的分割能力。当在无推理数据集上进行训练时，LISA 在推理分割任务中表现出强大的零样本能力，并通过仅对涉及推理的 239 张图像指令对进行微调而进一步提高了性能。

#### LISA++

<img src="assets/image-20250706163536936.png" alt="image-20250706163536936" style="zoom:80%;" /> 

- 增强分割：添加了实例分割能力，提供了更详细的场景分析以及现有的多区域语义分割。
- 更自然的对话：提高多轮对话的能力，能够将分割结果直接合并到文本响应中，即对话 (SiD) 中的分割。
- 这些改进是通过管理通用分割数据集的现有样本来实现的，旨在专门增强分割和对话技能，而无需结构变化和额外的数据源。 

### SegLLM（ICLR2025）

[[2410.18923] SegLLM: Multi-round Reasoning Segmentation](https://arxiv.org/abs/2410.18923)

<img src="assets/image-20250707153911340.png" alt="image-20250707153911340" style="zoom: 67%;" /> 

### LLM-Seg（CVPR2024）

[[2404.08767] LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning](https://arxiv.org/abs/2404.08767)

<img src="assets/image-20250707234958688.png" alt="image-20250707234958688" style="zoom: 67%;" /> 

<img src="assets/image-20250708005702525.png" alt="image-20250708005702525" style="zoom:80%;" /> 

 

### OpenMaskDINO3D

[[2506.04837] OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model](https://arxiv.org/abs/2506.04837)

<img src="assets/image-20250706180932844.png" alt="image-20250706180932844" style="zoom:80%;" /> 



### MIRAS

[[2502.09447] Pixel-Level Reasoning Segmentation via Multi-turn Conversations](https://arxiv.org/abs/2502.09447)<img src="assets/image-20250706191141337.png" alt="image-20250706191141337" style="zoom:80%;" /> 



### HyperSeg

https://arxiv.org/abs/2411.17606

<img src="assets/image-20250707203130612.png" alt="image-20250707203130612" style="zoom:67%;" /> 

### See say and segment

[[2312.08366] See, Say, and Segment: Teaching LMMs to Overcome False Premises](https://arxiv.org/abs/2312.08366)<img src="assets/image-20250707235510866.png" alt="image-20250707235510866" style="zoom:80%;" /> 

# 评测指标

## 机器学习

### 二分类

<img src="assets/image-20250808170439418.png" alt="image-20250808170439418" style="zoom:50%;" /> 

- 精度/精确率：$acc=\frac{TP+TN}{TP+TN+FP+FN}$
- 准确率/查准率：$\mathrm{Precision}=\frac{TP}{TP+FP}$
- 召回率/查全率：$\mathrm{Recall}=\frac{TP}{TP+FN}$
- F1-score：$F1=\frac{2\cdot \mathrm{Precision}\cdot \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}=\frac{2TP}{2TP+FP+FN}$
- F-score：$F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{(\beta^2 \cdot \mathrm{Precision}) + \mathrm{Recall}}$
- F1-score 是 $\mathrm{Precision}$ 和 $\mathrm{Recall}$ 的调和平均，尽量让两种指标都很高

## 图像生成

### FID

## 自然语言生成(NLG)

### BLEU

**Bilingual Evaluation Understudy**

- 通过计算模型生成文本与一个或多个参考文本之间 n-gram 的重合程度来评估翻译质量。
- 主要关注精确率 (precision)，并引入了 brevity penalty 来惩罚过短的生成结果。

<img src="assets/image-20250808172554723.png" alt="image-20250808172554723" style="zoom:50%;" /> 



## 图像检测

### IoU

### CIoU

### GIoU

## 大语言模型

### 困惑度(perplexity)
