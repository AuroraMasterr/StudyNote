# 机器学习

## softmax 分类器

<img src="assets\image-20250410185519007.png" alt="image-20250410185519007" style="zoom:67%;" /> 

<img src="assets\image-20250410185608563.png" alt="image-20250410185608563" style="zoom:67%;" /> 

## SVM

<img src="assets\image-20250410185711881.png" alt="image-20250410185711881" style="zoom:67%;" /> 

## 精度

<img src="assets\image-20250423170702645.png" alt="image-20250423170702645" style="zoom:67%;" /> 

<img src="assets\image-20250423170753653.png" alt="image-20250423170753653" style="zoom:67%;" />  

<img src="assets\image-20250430200312137.png" alt="image-20250430200312137" style="zoom:67%;" /> 

<img src="assets\image-20250430200426708.png" alt="image-20250430200426708" style="zoom:67%;" /> 

<img src="assets\image-20250423171453105.png" alt="image-20250423171453105" style="zoom:67%;" />  

## 正则化

<img src="assets\image-20250410190811998.png" alt="image-20250410190811998" style="zoom:80%;" /> 

## 激活函数

<img src="assets\image-20250512194102742.png" alt="image-20250512194102742" style="zoom:67%;" /> 

<img src="assets\image-20250512194505385.png" alt="image-20250512194505385" style="zoom:67%;" /> 

<img src="assets\image-20250512195130587.png" alt="image-20250512195130587" style="zoom:67%;" /> 



## 优化器



## 概率图模型

<img src="assets\image-20250512011127763.png" alt="image-20250512011127763" style="zoom:80%;" /> 

<img src="assets\image-20250512011543407.png" alt="image-20250512011543407" style="zoom:80%;" /> 

<img src="assets\image-20250512011556497.png" alt="image-20250512011556497" style="zoom:80%;" /> 

### HHM

### CRF

## PCA降维

<img src="assets\image-20250512100347297.png" alt="image-20250512100347297" style="zoom:57%;" /> 

<img src="assets\image-20250512101043525.png" alt="image-20250512101043525" style="zoom: 55%;" /> 

<img src="assets\image-20250512101639950.png" alt="image-20250512101639950" style="zoom: 67%;" />   

<img src="assets\image-20250512094732647.png" alt="image-20250512094732647" style="zoom: 67%;" /> 

<img src="assets\image-20250512095515538.png" alt="image-20250512095515538" style="zoom:67%;" /> 

# 深度学习

## 自编码器

<img src="assets\image-20250423004223975.png" alt="image-20250423004223975" style="zoom:67%;" /> 



<img src="https://pic4.zhimg.com/v2-4cb07847643ff8b6e4dd612b68472d95_1440w.jpg" alt="img" style="zoom: 80%;" /> 

<img src="assets\image-20250423103034998.png" alt="image-20250423103034998" style="zoom:67%;" /> 

### 重参数化(reparameterization)

<img src="assets\image-20250423014516357.png" alt="image-20250423014516357" style="zoom:67%;" /> 

<img src="assets\image-20250423014556711.png" alt="image-20250423014556711" style="zoom:67%;" /> 

```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        # 编码器部分
        self.fc1 = nn.Linear(input_dim, 512)
        self.fc21 = nn.Linear(512, latent_dim)  # 均值
        # 由于方差不能为负，所以这里输出的是log(方差)，而不是方差本身.
        self.fc22 = nn.Linear(512, latent_dim)  # 对数方差
 
        # 解码器部分
        self.fc3 = nn.Linear(latent_dim, 512)
        self.fc4 = nn.Linear(512, input_dim)
 
    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)
 
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # 从标准正态分布N(0, 1)中采样
        return mu + eps * std  # 重参数化公式
 
    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))  # 使用sigmoid激活函数输出到[0, 1]区间
 
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

## 图神经网络GNN

<img src="assets\image-20250430191924314.png" alt="image-20250430191924314" style="zoom:67%;" /> 

### GCN

<img src="assets\image-20250430193404994.png" alt="image-20250430193404994" style="zoom:67%;" /> 

![image-20250430194802463](assets\image-20250430194802463.png) 

<img src="assets\image-20250430194211056.png" alt="image-20250430194211056" style="zoom:67%;" /> 

![image-20250430195237020](assets\image-20250430195237020.png) 

<img src="assets\image-20250430193316692.png" alt="image-20250430193316692" style="zoom: 80%;" />



## 卷积神经网络CNN

### 深度可分离卷积（DSC）

<img src="assets\image-20250512172809025.png" alt="image-20250512172809025" style="zoom:67%;" /> 

<img src="assets\image-20250512173606480.png" alt="image-20250512173606480" style="zoom:80%;" /> 

<img src="assets\image-20250512173945141.png" alt="image-20250512173945141" style="zoom:67%;" /> 

<img src="assets\image-20250512174016883.png" alt="image-20250512174016883" style="zoom:67%;" /> 

# Transformer架构

## Transformer

<img src="assets\image-20250514022040178.png" alt="image-20250514022040178" style="zoom:67%;" /> 



## BERT

## GPT

## Attention

### Self Attention

<img src="assets\image-20250516205208397.png" alt="image-20250516205208397" style="zoom:67%;" />  



### Cross Attention

与 Self-Attention 不同，Self-Attention 是让一个序列自己内部的元素相互关注（比如一个句子中的单词互相计算关系），而 Cross Attention 则是让**两个不同的序列**之间建立关注关系。

Cross Attention 的核心在于：它允许一个序列（称为 Query，查询）去关注另一个序列（称为 Key 和 Value，键和值），从而实现信息的融合。

<img src="assets\image-20250516205343160.png" alt="image-20250516205343160" style="zoom:67%;" /> 

<img src="assets\image-20250516205834217.png" alt="image-20250516205834217" style="zoom:67%;" /> 



### MQA

**(Multi-Query Attention)**

MQA 让所有的头之间 **共享** 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。

我们知道了 MQA 实际上是将 head 中的 key 和 value 矩阵抽出来单独存为一份共享参数，而 query 则是依旧保留在原来的 head 中，每个 head 有一份自己独有的 query 参数。

### GQA

**(Grouped-Query Attention)**

GQA将查询头分成G组，对于Query是每个头单独保留一份参数，每个组共享一个Key 和 Value 矩阵。GQA-G是指具有G组的grouped-query attention。

中间组数导致插值模型的质量高于 MQA，但比 MHA 更快。从 MHA 到 MQA 将 H 键和值头减少到单个键和值头，减少了键值缓存的大小，因此需要加载的数据量 H 倍。

<img src="assets\image-20250517014338254.png" alt="image-20250517014338254" style="zoom:67%;" /> 

### MLA

**(Multi-Head Latent Attention)**

### FlashAttention

[Flash Attention论文解读 - 李理的博客](https://fancyerii.github.io/2023/10/23/flashattention/)

<img src="assets\image-20250520020712996.png" alt="image-20250520020712996" style="zoom:67%;" /> 

**核心思想**：传统减少HBM的访问，将QKV切分为小块后放入SRAM中

**核心方法**：tiling, recomputation 

FlashAttention旨在**加速**注意力计算并**减少内存占用**。FlashAttention利用底层硬件的内存层次知识，例如GPU的内存层次结构，来提高计算速度和减少内存访问开销。 FlashAttention的核心原理是通过将输入**分块**并在每个块上执行注意力操作，从而减少对高带宽内存（HBM）的读写操作。

具体而言，FlashAttention 使用平铺和重计算等经典技术，将输入块从HBM加载到 SRAM（快速缓存），在SRAM上执行注意力操作，并将结果更新回HBM。FlashAttention减少了内存读写量，从而实现了**2-4倍** 的时钟时间加速。

<img src="assets\image-20250520015144621.png" alt="image-20250520015144621" style="zoom:67%;" /> 

<img src="assets\image-20250520015118296.png" alt="image-20250520015118296" style="zoom:67%;" /> 

<img src="assets\image-20250520020743919.png" alt="image-20250520020743919" style="zoom: 67%;" /> 

# 强化学习

<img src="assets\image-20250423174556408.png" alt="image-20250423174556408" style="zoom:67%;" /> 

<img src="assets\image-20250423174611669.png" alt="image-20250423174611669" style="zoom:50%;" /> 

<img src="assets\image-20250423174754609.png" alt="image-20250423174754609" style="zoom:67%;" /> 

## DQN

## PPO

# 计算机视觉(CV)

## 超分辨率Super-Resolution

<img src="assets\image-20250403230441211.png" alt="image-20250403230441211" style="zoom:80%;" /> 

### 插值

<img src="assets\image-20250403223119126.png" alt="image-20250403223119126" style="zoom:67%;" /> 

<img src="assets\image-20250403221621550.png" alt="image-20250403221621550" style="zoom: 67%;" /> 

<img src="assets\image-20250403221651797.png" alt="image-20250403221651797" style="zoom:80%;" /> 

<img src="assets\image-20250403222350685.png" alt="image-20250403222350685" style="zoom:67%;" /> 

<img src="assets\image-20250403222407214.png" alt="image-20250403222407214" style="zoom:67%;" /> 

<img src="assets\image-20250403222500027.png" alt="image-20250403222500027" style="zoom:67%;" /> 

<img src="assets\image-20250403222851881.png" alt="image-20250403222851881" style="zoom:67%;" />  

<img src="assets\image-20250403230104523.png" alt="image-20250403230104523" style="zoom:67%;" /> 

<img src="assets\image-20250403230120013.png" alt="image-20250403230120013" style="zoom:80%;" />  

<img src="assets\image-20250403223101220.png" alt="image-20250403223101220" style="zoom:67%;" /> 

<img src="assets\image-20250403225647531.png" alt="image-20250403225647531" style="zoom:67%;" /> 

## 图像分割

### 语义分割



### 实例分割

## 目标检测

### yolo

### SSD

### faster RCNN



# 自然语言处理(NLP)

## TF-IDF

<img src="assets\image-20250517175934886.png" alt="image-20250517175934886" style="zoom:80%;" /> 

<img src="assets\image-20250517180016963.png" alt="image-20250517180016963" style="zoom:67%;" /> 

 <img src="assets\image-20250517180244359.png" alt="image-20250517180244359" style="zoom:67%;" /> 

<img src="assets\image-20250517182015899.png" alt="image-20250517182015899" style="zoom:67%;" /> 

 <img src="assets\image-20250517182311893.png" alt="image-20250517182311893" style="zoom:67%;" /> 

TF−IWF 这种加权方法 **降低了文档集/语料库中同类文本对词权重的影响，更加精确地表达了词在待查文档中的重要程度**。

传统 TF−IDF 所求的权值一般很小，甚至接近于 0，精确度也不高，而 TF−IWF 的计算结果恰能解决权值过小的问题。

## LDA 主题模型

<img src="assets\image-20250517222951453.png" alt="image-20250517222951453" style="zoom: 80%;" /> 

<img src="assets\image-20250518021410497.png" alt="image-20250518021410497" style="zoom:80%;" /> 

## Tokenizer

 <img src="assets\image-20250519001025901.png" alt="image-20250519001025901" style="zoom:67%;" /> 

<img src="assets\image-20250519001402351.png" alt="image-20250519001402351" style="zoom: 67%;" /> 

<img src="assets\image-20250519001448704.png" alt="image-20250519001448704" style="zoom:67%;" /> 

<img src="assets\image-20250519001841783.png" alt="image-20250519001841783" style="zoom:67%;" /> 

<img src="assets\image-20250519005132956.png" alt="image-20250519005132956" style="zoom:67%;" /> 

 <img src="assets\image-20250519005146975.png" alt="image-20250519005146975" style="zoom: 80%;" /> 

<img src="assets\image-20250519003305498.png" alt="image-20250519003305498" style="zoom:67%;" /> 

### BPE

<img src="assets\image-20250519002401003.png" alt="image-20250519002401003" style="zoom:67%;" /> 

<img src="assets\image-20250519005314906.png" alt="image-20250519005314906" style="zoom:80%;" />  

<img src="assets\image-20250519004346279.png" alt="image-20250519004346279" style="zoom: 80%;" /> 

<img src="assets\image-20250519004607890.png" alt="image-20250519004607890" style="zoom:80%;" /> 

### WordPiece

<img src="assets\image-20250519005951093.png" alt="image-20250519005951093" style="zoom:80%;" /> 

### Unigram

<img src="assets\image-20250519010829154.png" alt="image-20250519010829154" style="zoom:80%;" /> 

<img src="assets\image-20250519010742662.png" alt="image-20250519010742662" style="zoom:80%;" /> 

<img src="assets\image-20250519011215732.png" alt="image-20250519011215732" style="zoom:80%;" /> 

# 分布式训练

## DP 和 DDP

<img src="assets\image-20250430134619762.png" alt="image-20250430134619762" style="zoom:67%;" /> 

<img src="assets\image-20250430135137120.png" alt="image-20250430135137120" style="zoom:67%;" /> 

<img src="assets\image-20250430180050311.png" alt="image-20250430180050311" style="zoom:67%;" /> 

<img src="assets\image-20250430180835259.png" alt="image-20250430180835259" style="zoom:80%;" /> 

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms

# 定义模型（示例用简单的线性模型）
class MyModel(torch.nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = torch.nn.Linear(10, 10)
    def forward(self, x):
        return self.fc(x)

# 训练函数
def train(rank, world_size):
    # 初始化进程组
    # 常见的通信后端包括 gloo 和 nccl，其中 nccl 是在 GPU 上最常用的后端
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)

    # 设置当前 GPU
    torch.cuda.set_device(rank)  # 若是在多机上，此处设置为local_rank

    # 创建模型并转移到当前 GPU
    model = MyModel().cuda(rank)  # 若是在多机上，此处设置为local_rank

    # 将模型包装为 DDP 模型
    model = DDP(model, device_ids=[rank])	# 若是在多机上，此处设置为local_rank

    # 定义损失函数和优化器
    criterion = torch.nn.CrossEntropyLoss().cuda(rank)	# 若是在多机上，此处设置为local_rank
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    # 加载数据集（示例使用 CIFAR10）
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

    # 使用 DistributedSampler 确保每个 GPU 得到不同的数据
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)	# 若是在多机上，此处设置为local_rank
    train_loader = DataLoader(dataset=train_dataset, batch_size=32, sampler=train_sampler)

    # 开始训练
    for epoch in range(10):
        model.train()
        train_sampler.set_epoch(epoch)  # 每个 epoch 设置随机种子以保证不同进程之间的数据一致
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.cuda(rank), target.cuda(rank)	# 若是在多机上，此处设置为local_rank
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # 销毁进程组
    dist.destroy_process_group()

# 主函数
def main():
    world_size = 2  # 使用 2 张 GPU

    # 使用 multiprocessing 启动多进程，每个 GPU 对应一个进程
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

<img src="assets\image-20250430180610671.png" alt="image-20250430180610671" style="zoom: 80%;" /> 

<img src="assets\image-20250430180641620.png" alt="image-20250430180641620" style="zoom:80%;" /> 

# 推荐系统

<img src="assets\image-20250517192945429.png" alt="image-20250517192945429" style="zoom:67%;" /> 

<img src="assets\image-20250517193115047.png" alt="image-20250517193115047" style="zoom: 80%;" /> 

<img src="assets\image-20250517193147276.png" alt="image-20250517193147276" style="zoom: 80%;" /> 

## 召回

<img src="assets\image-20250517193240590.png" alt="image-20250517193240590" style="zoom:80%;" /> 

# 模型轻量化

## 蒸馏

<img src="assets\image-20250423153200155.png" alt="image-20250423153200155" style="zoom:67%;" /> 

<img src="assets\image-20250423153133783.png" alt="image-20250423153133783" style="zoom:80%;" /> 

<img src="assets\image-20250423153745152.png" alt="image-20250423153745152" style="zoom:67%;" /> 

<img src="assets\image-20250423153759589.png" alt="image-20250423153759589" style="zoom:67%;" /> 

<img src="assets\image-20250423153301493.png" alt="image-20250423153301493" style="zoom: 67%;" /> 

<img src="assets\image-20250423153634116.png" alt="image-20250423153634116" style="zoom: 67%;" /> 

## 量化

<img src="assets\image-20250512221958977.png" alt="image-20250512221958977" style="zoom: 50%;" /> 

<img src="assets\image-20250512223845596.png" alt="image-20250512223845596" style="zoom:80%;" /> 

<img src="assets\image-20250512223751019.png" alt="image-20250512223751019" style="zoom: 67%;" /> 

<img src="assets\image-20250512223812038.png" alt="image-20250512223812038" style="zoom:67%;" /> 

<img src="assets\image-20250512223822901.png" alt="image-20250512223822901" style="zoom: 67%;" /> 

###  训练后量化(PTQ)

**Post-training-quantization**

<img src="assets\image-20250512224531232.png" alt="image-20250512224531232" style="zoom: 67%;" /> 

<img src="assets\image-20250512224557276.png" alt="image-20250512224557276" style="zoom: 67%;" /> 

### 量化感知训练(QAT)

**Quantization-aware-training**

<img src="assets\image-20250514012604076.png" alt="image-20250514012604076" style="zoom:67%;" /> 

## 低秩分解

# 大语言模型



## 微调

### 低秩适应(LoRA)

Low-Rank Adaptation

<img src="assets\image-20250514020113800.png" alt="image-20250514020113800" style="zoom:80%;" /> 

<img src="assets\image-20250514021140044.png" alt="image-20250514021140044" style="zoom: 67%;" /> 

<img src="assets\image-20250514021009857.png" alt="image-20250514021009857" style="zoom:67%;" /> 

## Tool Learning

<img src="assets\image-20250515103539089.png" alt="image-20250515103539089" style="zoom:67%;" /> 

<img src="assets\image-20250515103951754.png" alt="image-20250515103951754" style="zoom: 67%;" /> 

<img src="assets\image-20250515104113191.png" alt="image-20250515104113191" style="zoom: 67%;" /> 

<img src="assets\image-20250515102328271.png" alt="image-20250515102328271" style="zoom:67%;" /> 

<img src="assets\image-20250515104237189.png" alt="image-20250515104237189" style="zoom:67%;" /> 

 <img src="assets\image-20250515104344725.png" alt="image-20250515104344725" style="zoom:67%;" /> 



## 多模态

<img src="assets\image-20250407132547580.png" alt="image-20250407132547580" style="zoom: 67%;" /> 

### CLIP

![image-20250407132319259](assets\image-20250407132319259.png) 

<img src="assets\image-20250407121825805.png" alt="image-20250407121825805" style="zoom:67%;" /> 

<img src="assets\image-20250407121854146.png" alt="image-20250407121854146" style="zoom:67%;" /> 

![image-20250407121957171](assets\image-20250407121957171.png) 

#### 预训练

<img src="assets\image-20250407122027474.png" alt="image-20250407122027474" style="zoom:67%;" /> 

<img src="assets\image-20250407122045422.png" alt="image-20250407122045422" style="zoom:67%;" /> 

<img src="assets\image-20250407122115156.png" alt="image-20250407122115156" style="zoom:80%;" />

#### 推理

![image-20250407122205488](assets\image-20250407122205488.png)

<img src="assets\image-20250407122422622.png" alt="image-20250407122422622" style="zoom:80%;" />

<img src="assets\image-20250407123811712.png" alt="image-20250407123811712" style="zoom:67%;" /> 

### 对比学习

<img src="assets\image-20250407125751136.png" alt="image-20250407125751136" style="zoom:67%;" /> 

<img src="assets\image-20250407125808900.png" alt="image-20250407125808900" style="zoom:80%;" /> 

<img src="assets\image-20250407125834294.png" alt="image-20250407125834294" style="zoom:67%;" /> 

<img src="assets\image-20250407130218548.png" alt="image-20250407130218548" style="zoom:67%;" /> 

<img src="assets\image-20250407130709955.png" alt="image-20250407130709955"  /> <img src="assets\image-20250407130721905.png" alt="image-20250407130721905" style="zoom:67%;" />  

<img src="assets\image-20250407130751914.png" alt="image-20250407130751914" style="zoom: 67%;" /> 

<img src="assets\image-20250407130853721.png" alt="image-20250407130853721" style="zoom:67%;" /> 

### CLAP

<img src="assets\image-20250502174823331.png" alt="image-20250502174823331" style="zoom:80%;" /> 

### BLIP

### VLM



## MoE 架构

[A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

<img src="assets\image-20250505183854419.png" alt="image-20250505183854419" style="zoom: 50%;" /> <img src="assets\image-20250505183955344.png" alt="image-20250505183955344" style="zoom:50%;" />   

<img src="assets\image-20250505182240868.png" alt="image-20250505182240868" style="zoom:67%;" /> 

<img src="assets\image-20250505182248298.png" alt="image-20250505182248298" style="zoom:67%;" /> 

<img src="assets\image-20250505182657503.png" alt="image-20250505182657503" style="zoom:67%;" /> 

<img src="assets\image-20250505182959308.png" alt="image-20250505182959308" style="zoom: 67%;" /> 

<img src="assets\image-20250505182947162.png" alt="image-20250505182947162" style="zoom:67%;" /> 

路由器（或门控网络）本身也是一个 FFN，它根据特定的输入选择专家。路由器会输出概率值，并利用这些概率来选择最匹配的专家：

**专家层返回被选定专家的输出，并乘以门控值（选择概率）。**

路由器和专家（其中仅选择少部分）共同构成了 MoE 层。

给定的 MoE 层有两种类型：稀疏专家混合模型（Sparse Mixture of Experts）和密集专家混合模型（Dense Mixture of Experts）。

两者都使用路由器来选择专家，但稀疏 MoE 只选择少数几个专家，而密集 MoE 则选择全部专家，但可能会以不同的分布进行选择。

<img src="assets\image-20250505184157768.png" alt="image-20250505184157768" style="zoom: 67%;" /> 

### 负载均衡

<img src="assets\image-20250505192124109.png" alt="image-20250505192124109" style="zoom:50%;" /> 

然而，这个简单的功能往往会导致路由器总是选择相同的专家，因为某些专家可能比其他专家学习得更快。

这不仅会导致专家选择的不均匀分布，还会导致某些专家几乎没有被训练过。这会在训练和推理过程中引发问题。

因此，我们希望在训练和推理期间，各个专家的使用具有同等的重要性，这就是所谓的负载平衡。某种程度上，这是为了防止模型在同一组专家上过拟合。

#### KeepTopK

一种对路由器进行负载平衡的方法是使用一个简单的扩展策略，称为 KeepTopK。通过引入可训练的（高斯）噪声，我们可以防止总是选择相同的专家：

<img src="assets\image-20250505185544253.png" alt="image-20250505185544253" style="zoom: 67%;" /> 

然后，除希望激活的前 k 个专家（例如 2 个）以外的所有专家权重都将被设为 $-\infty$。

将这些权重设为 $-\infty$ 时，SoftMax 操作后的输出概率将变为 0。

<img src="assets\image-20250505194542278.png" alt="image-20250505194542278" style="zoom: 67%;" /> 



#### 辅助损失

#### 专家容量

### Switch Transformer

### GShard

### Vision-MoE

### 优势

- **提高模型性能**：通过将多个专家的预测结果进行整合，MoE 模型可以在不同的数据子集或任务方面发挥每个专家的优势，从而提高整体模型的性能。例如，在图像分类任务中，一个专家可能擅长识别动物图片，而另一个专家可能擅长识别车辆图片，通过门控网络的合理分配，MoE 模型可以更准确地对不同类型的图片进行分类。
- **减少计算成本**：与传统的密集模型相比，MoE 模型在处理每个输入样本时，只有相关的专家会被激活，而不是整个模型的所有参数都被使用。这意味着 MoE 模型可以在保持较高性能的同时，显著减少计算资源的消耗，特别是在模型规模较大时，这种优势更为明显。例如，对于一个具有数十亿参数的大型语言模型，采用 MoE 架构可以在不增加太多计算成本的情况下，通过增加专家的数量来进一步提升模型的性能。
- **增强模型的可扩展性**：MoE 模型的架构设计使得它可以很容易地扩展到更多的专家和更大的模型规模。通过增加专家的数量，模型可以覆盖更广泛的数据特征和任务类型，从而在不增加计算复杂度的情况下，提升模型的表达能力和泛化能力。这种可扩展性为处理大规模、复杂的数据集提供了有效的解决方案，例如在处理多模态数据（包含文本、图像、语音等多种类型的数据）时，MoE 模型可以通过设置不同的专家来专门处理不同模态的数据，实现更高效的多模态融合。

### 挑战

- 计算成本与资源管理
  - **内存需求高**：MoE模型需要将所有专家的参数都加载到内存中，即使在推理过程中只使用其中一部分专家。例如，以Mixtral 8x7B这样的MoE模型为例，需要有足够的VRAM来容纳一个47B参数的稠密模型。这是因为MoE模型中只有FFN层被视为独立的专家，而模型的其他参数是共享的。高内存需求使得在资源有限的情况下部署和运行MoE模型变得困难，特别是在需要处理大规模参数模型时，对硬件设备的要求更为苛刻。
  - **分布式训练复杂**：为了应对大规模模型的训练，通常需要采用分布式训练的方式。但在MoE模型中，由于专家之间的数据交换和并行训练需要机间all-to-all通信来实现，这增加了通信成本。随着模型规模的增大，通信开销也相应增加，可能导致训练效率降低。例如，在一个大规模分布式训练场景中，若模型参数规模达到数十亿甚至更大，通信延迟和网络拥塞问题可能会严重影响训练速度和性能。因此，在实际部署过程中，需要仔细设计通信策略和优化网络拓扑，以降低通信延迟和潜在的网络拥塞。
  - **专家容量限制**：为了防止特定专家过载并确保工作负载平衡，通常会对每个专家可以同时处理的输入数量设置阈值。例如，采用top-2路由和1.25的容量因子，这意味着每个输入选择两个专家，每个专家处理其通常容量的1.25倍。这种策略虽然可以在一定程度上平衡负载，但也可能导致部分数据无法及时处理或需要重新分配，影响训练和推理的效率。此外，专家容量的设置需要根据具体的任务和模型规模进行调整，这增加了模型配置和管理的复杂性。
- 过拟合与泛化问题
  - **过拟合风险**：与稠密模型相比，MoE模型在微调时更易产生过拟合现象。这是因为MoE模型的参数量虽然大，但在实际应用中只激活部分专家，模型的复杂度相对较高。例如，拥有1.6T参数量的MoE预训练模型Switch Transformer，在SuperGLUE等常见基准上进行微调时，其整体性能却落后于较小的模型。这表明在微调过程中，模型可能会过度拟合训练数据中的噪声和细节，而无法很好地泛化到新的、未见过的数据上。
  - **泛化能力不足**：MoE模型的泛化能力在某些任务上表现不佳，尤其是在需要对输入数据进行深入理解和推理的任务中。例如，在重理解任务（如SuperGLUE）上，MoE模型的表现不如对应的稠密模型。这可能是因为MoE模型在训练过程中，专家之间的协作和知识共享不够充分，导致模型对特定任务的理解和处理能力有限。此外，门控网络的设计和训练也可能影响模型的泛化能力，如果门控网络不能准确地将输入数据分配给最合适的专家，就会影响模型的整体性能。
  - **微调策略选择**：为了提高MoE模型在微调阶段的泛化能力，需要选择合适的微调策略。一种可行的方法是尝试冻结所有非专家层的权重，只对MoE层的参数进行更新。实验结果显示，这种方法几乎与更新所有参数的效果相当，同时可以加速微调过程并降低显存需求。此外，使用较小的批量大小和较高的学习率进行微调，也有助于提高模型的泛化性能。然而，这些策略的选择需要根据具体的任务和模型情况进行调整，不同的任务可能需要不同的微调策略来达到最佳效果。
