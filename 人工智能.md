# 机器学习

## softmax 分类器

<img src="assets\image-20250410185519007.png" alt="image-20250410185519007" style="zoom:67%;" /> 

<img src="assets\image-20250410185608563.png" alt="image-20250410185608563" style="zoom:67%;" /> 

## SVM

<img src="assets\image-20250410185711881.png" alt="image-20250410185711881" style="zoom:67%;" /> 

## 决策树

### 算法

<img src="assets/image-20250721123248643.png" alt="image-20250721123248643" style="zoom:50%;" /> 

显然，决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会导致递归返回: 

(1) 当前结点包含的样本全属于同一类别，无需划分; 

(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; 

(3) 当前结点包含的样本集合为空，不能划分.

在第(2)种情形下，我们把当前结点标记为叶结点，井将其类别设定为该结点所含样本最多的类别;在第 (3) 种情形下，同样把当前结点标记为叶结点，将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不同：情形(2) 是在利用当前结点的后验分布，而情形(3) 则是把父结点的样本分布作为当前结点的先验分布. 

### 信息增益

<img src="assets/image-20250726002013120.png" alt="image-20250726002013120" style="zoom:80%;" /> 

### 基尼指数

<img src="assets/image-20250726002224034.png" alt="image-20250726002224034" style="zoom:80%;" /> 

### 剪枝  

 

## 精度

<img src="assets\image-20250423170702645.png" alt="image-20250423170702645" style="zoom:67%;" /> 

<img src="assets\image-20250423170753653.png" alt="image-20250423170753653" style="zoom:67%;" />  

<img src="assets\image-20250430200312137.png" alt="image-20250430200312137" style="zoom:67%;" /> 

<img src="assets\image-20250430200426708.png" alt="image-20250430200426708" style="zoom:67%;" /> 

<img src="assets\image-20250423171453105.png" alt="image-20250423171453105" style="zoom:67%;" />  

## 正则化（Regularization）

<img src="assets\image-20250410190811998.png" alt="image-20250410190811998" style="zoom:80%;" /> 

## 归一化（Normalization）

### RMS Norm

**Root mean square Norm**

![image-20250701162050277](assets/image-20250701162050277.png)

![image-20250701162326945](assets/image-20250701162326945.png) 

### Batch Norm

- 在推理时的均值方差需要固定，用的是训练时所有均值方差的 EMA

## 激活函数

### Sigmoid

<img src="assets\image-20250512194102742.png" alt="image-20250512194102742" style="zoom:67%;" /> 

### Tanh

<img src="assets\image-20250512194505385.png" alt="image-20250512194505385" style="zoom:67%;" />  

### Swish

<img src="assets/image-20250720174318662.png" alt="image-20250720174318662" style="zoom:50%;" /> 

<img src="assets/image-20250720174451060.png" alt="image-20250720174451060" style="zoom:80%;" /> 

### GELU

![image-20250701171959179](assets/image-20250701171959179.png) 

<img src="assets/image-20250701171728437.png" alt="image-20250701171728437" style="zoom:50%;" /> 

### SwiGLU

<img src="assets/image-20250701174055352.png" alt="image-20250701174055352" style="zoom:50%;" /> 

<img src="assets/image-20250701174325050.png" alt="image-20250701174325050" style="zoom: 50%;" /> 

## 优化器

### SGD

### Adam

## 聚类

### 外部指标

将聚类结果与某个"参考模型"进行比较

<img src="assets/image-20250723174048923.png" alt="image-20250723174048923" style="zoom:50%;" /> 

<img src="assets/image-20250723174107834.png" alt="image-20250723174107834" style="zoom:50%;" /> 

<img src="assets/image-20250723174121450.png" alt="image-20250723174121450" style="zoom:50%;" /> 

### 内部指标

直接考察聚类结果而不利用任何参考模型。

<img src="assets/image-20250723174223241.png" alt="image-20250723174223241" style="zoom:50%;" /> 

<img src="assets/image-20250723174236951.png" alt="image-20250723174236951" style="zoom:50%;" /> 

### k 均值算法

<img src="assets/image-20250723174330631.png" alt="image-20250723174330631" style="zoom:50%;" /> 

<img src="assets/image-20250723174459643.png" alt="image-20250723174459643" style="zoom: 50%;" /> 

### 学习向量量化(LVQ)

**Learning Vector Quantization**

<img src="assets/image-20250723174656398.png" alt="image-20250723174656398" style="zoom:50%;" /> 

### DBSCAN

**Density-Based Spatial Clustering of Applications with Noise**

<img src="assets/image-20250723175037118.png" alt="image-20250723175037118" style="zoom:50%;" /> 

<img src="assets/image-20250723175058471.png" alt="image-20250723175058471" style="zoom:50%;" /> 

<img src="assets/image-20250723175113966.png" alt="image-20250723175113966" style="zoom:50%;" /> 

## 概率图模型

<img src="assets\image-20250512011127763.png" alt="image-20250512011127763" style="zoom:80%;" /> 

<img src="assets\image-20250512011543407.png" alt="image-20250512011543407" style="zoom:80%;" /> 

<img src="assets\image-20250512011556497.png" alt="image-20250512011556497" style="zoom:80%;" /> 

### HHM

### CRF

## PCA降维

<img src="assets\image-20250512100347297.png" alt="image-20250512100347297" style="zoom:57%;" /> 

<img src="assets\image-20250512101043525.png" alt="image-20250512101043525" style="zoom: 55%;" /> 

<img src="assets\image-20250512101639950.png" alt="image-20250512101639950" style="zoom: 67%;" />   

<img src="assets\image-20250512094732647.png" alt="image-20250512094732647" style="zoom: 67%;" /> 

<img src="assets\image-20250512095515538.png" alt="image-20250512095515538" style="zoom:67%;" /> 



# 深度学习

## 自编码器

<img src="assets\image-20250423004223975.png" alt="image-20250423004223975" style="zoom:67%;" /> 



<img src="https://pic4.zhimg.com/v2-4cb07847643ff8b6e4dd612b68472d95_1440w.jpg" alt="img" style="zoom: 80%;" /> 

<img src="assets\image-20250423103034998.png" alt="image-20250423103034998" style="zoom:67%;" /> 

### 重参数化(reparameterization)

<img src="assets\image-20250423014516357.png" alt="image-20250423014516357" style="zoom:67%;" /> 

<img src="assets\image-20250423014556711.png" alt="image-20250423014556711" style="zoom:67%;" /> 

```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        # 编码器部分
        self.fc1 = nn.Linear(input_dim, 512)
        self.fc21 = nn.Linear(512, latent_dim)  # 均值
        # 由于方差不能为负，所以这里输出的是log(方差)，而不是方差本身.
        self.fc22 = nn.Linear(512, latent_dim)  # 对数方差
 
        # 解码器部分
        self.fc3 = nn.Linear(latent_dim, 512)
        self.fc4 = nn.Linear(512, input_dim)
 
    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)
 
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # 从标准正态分布N(0, 1)中采样
        return mu + eps * std  # 重参数化公式
 
    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))  # 使用sigmoid激活函数输出到[0, 1]区间
 
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```

## 图神经网络GNN

<img src="assets\image-20250430191924314.png" alt="image-20250430191924314" style="zoom:67%;" /> 

### GCN

<img src="assets\image-20250430193404994.png" alt="image-20250430193404994" style="zoom:67%;" /> 

![image-20250430194802463](assets\image-20250430194802463.png) 

<img src="assets\image-20250430194211056.png" alt="image-20250430194211056" style="zoom:67%;" /> 

![image-20250430195237020](assets\image-20250430195237020.png) 

<img src="assets\image-20250430193316692.png" alt="image-20250430193316692" style="zoom: 80%;" />



## 扩散模型

### DDPM



## 卷积神经网络CNN

### 深度可分离卷积（DSC）

<img src="assets\image-20250512172809025.png" alt="image-20250512172809025" style="zoom:67%;" /> 

<img src="assets\image-20250512173606480.png" alt="image-20250512173606480" style="zoom:80%;" /> 

<img src="assets\image-20250512173945141.png" alt="image-20250512173945141" style="zoom:67%;" /> 

<img src="assets\image-20250512174016883.png" alt="image-20250512174016883" style="zoom:67%;" /> 

# Transformer架构

## Transformer

<img src="assets\image-20250514022040178.png" alt="image-20250514022040178" style="zoom:67%;" /> 



## BERT

**Bidirectional Encoder Representation from Transformers**

<img src="assets/image-20250701175312162.png" alt="image-20250701175312162" style="zoom:67%;" /> 

Embedding由三种Embedding求和而成：

Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务

Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务

Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的

<img src="assets/image-20250701185220489.png" alt="image-20250701185220489" style="zoom: 67%;" /> 

<img src="assets/image-20250701202711001.png" alt="image-20250701202711001" style="zoom:67%;" /> 

![image-20250701202740359](assets/image-20250701202740359.png)

```python
class BERTLM(nn.Module):
    """
    BERT Language Model
    Next Sentence Prediction Model + Masked Language Model
    """

    def __init__(self, bert: BERT, vocab_size):
        """
        :param bert: BERT model which should be trained
        :param vocab_size: total vocab size for masked_lm
        """

        super().__init__()
        self.bert = bert
        self.next_sentence = NextSentencePrediction(self.bert.hidden)
        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)

    def forward(self, x, segment_label):
        x = self.bert(x, segment_label)
        return self.next_sentence(x), self.mask_lm(x)


class NextSentencePrediction(nn.Module):
    """
    2-class classification model : is_next, is_not_next
    """

    def __init__(self, hidden):
        """
        :param hidden: BERT model output size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, 2)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x[:, 0]))


class MaskedLanguageModel(nn.Module):
    """
    predicting origin token from masked input sequence
    n-class classification problem, n-class = vocab_size
    """

    def __init__(self, hidden, vocab_size):
        """
        :param hidden: output size of BERT model
        :param vocab_size: total vocab size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x))
```

## GPT

## Attention

### Self Attention

<img src="assets\image-20250516205208397.png" alt="image-20250516205208397" style="zoom:67%;" />  



### Cross Attention

与 Self-Attention 不同，Self-Attention 是让一个序列自己内部的元素相互关注（比如一个句子中的单词互相计算关系），而 Cross Attention 则是让**两个不同的序列**之间建立关注关系。

Cross Attention 的核心在于：它允许一个序列（称为 Query，查询）去关注另一个序列（称为 Key 和 Value，键和值），从而实现信息的融合。

<img src="assets\image-20250516205343160.png" alt="image-20250516205343160" style="zoom:67%;" /> 

<img src="assets\image-20250516205834217.png" alt="image-20250516205834217" style="zoom:67%;" /> 



### MQA

**(Multi-Query Attention)**

MQA 让所有的头之间 **共享** 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。

我们知道了 MQA 实际上是将 head 中的 key 和 value 矩阵抽出来单独存为一份共享参数，而 query 则是依旧保留在原来的 head 中，每个 head 有一份自己独有的 query 参数。

### GQA

**(Grouped-Query Attention)**

GQA将查询头分成G组，对于Query是每个头单独保留一份参数，每个组共享一个Key 和 Value 矩阵。GQA-G是指具有G组的grouped-query attention。

中间组数导致插值模型的质量高于 MQA，但比 MHA 更快。从 MHA 到 MQA 将 H 键和值头减少到单个键和值头，减少了键值缓存的大小，因此需要加载的数据量 H 倍。

<img src="assets\image-20250517014338254.png" alt="image-20250517014338254" style="zoom:67%;" /> 

### MLA

**(Multi-Head Latent Attention)**

### FlashAttention

[Flash Attention论文解读 - 李理的博客](https://fancyerii.github.io/2023/10/23/flashattention/)

<img src="assets\image-20250520020712996.png" alt="image-20250520020712996" style="zoom:67%;" /> 

**核心思想**：传统减少HBM的访问，将QKV切分为小块后放入SRAM中

**核心方法**：tiling, recomputation 

FlashAttention旨在**加速**注意力计算并**减少内存占用**。FlashAttention利用底层硬件的内存层次知识，例如GPU的内存层次结构，来提高计算速度和减少内存访问开销。 FlashAttention的核心原理是通过将输入**分块**并在每个块上执行注意力操作，从而减少对高带宽内存（HBM）的读写操作。

具体而言，FlashAttention 使用平铺和重计算等经典技术，将输入块从HBM加载到 SRAM（快速缓存），在SRAM上执行注意力操作，并将结果更新回HBM。FlashAttention减少了内存读写量，从而实现了**2-4倍** 的时钟时间加速。

<img src="assets\image-20250520015144621.png" alt="image-20250520015144621" style="zoom:67%;" /> 

<img src="assets\image-20250520015118296.png" alt="image-20250520015118296" style="zoom:67%;" /> 

<img src="assets\image-20250520020743919.png" alt="image-20250520020743919" style="zoom: 67%;" /> 

## Swin Transformer

<img src="assets/image-20250704204931292.png" alt="image-20250704204931292" style="zoom:80%;" /> 

 <img src="assets/image-20250704205134536.png" alt="image-20250704205134536" style="zoom:67%;" /> 

 ## Vision Transformer

[[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

<img src="assets/image-20250706161732340.png" alt="image-20250706161732340" style="zoom:80%;" /> 

- ViT-H（High resolution）：ViT-H是Vision Transformer模型中的高分辨率变体。它通常适用于处理高分辨率图像或更具挑战性的视觉任务。由于处理高分辨率图像可能需要更多的计算资源和内存，因此ViT-H模型可能更庞大和复杂。
- ViT-L（Low resolution）：ViT-L是Vision Transformer模型中的低分辨率变体。它通常用于处理低分辨率图像或资源受限的环境。ViT-L模型可能比ViT-H模型更小、更轻量级，适合在资源受限的设备或场景中部署。
- ViT-B（Base resolution）：ViT-B是Vision Transformer模型中的基准分辨率变体。它可以被视为ViT模型的中间规模。ViT-B通常是指在资源充足但不需要处理过高或过低分辨率图像时使用的模型。

## 位置编码

### RoPE

https://arxiv.org/abs/2104.09864

<img src="assets/image-20250715164636630.png" alt="image-20250715164636630" style="zoom:67%;" />

<img src="assets/image-20250715164457895.png" alt="image-20250715164457895" style="zoom:67%;" /> 

``` python
def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):
    # 计算词向量元素两两分组之后，每组元素对应的旋转角度
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]
    t = torch.arange(seq_len, device=freqs.device)
    # freqs.shape = [seq_len, dim // 2] 
    freqs = torch.outer(t, freqs).float()
    # torch.polar 的文档
    # https://pytorch.org/docs/stable/generated/torch.polar.html
    # 计算结果是个复数向量
    # 假设 freqs = [x, y]
    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    # xq.shape = [batch_size, seq_len, dim]
    # xq_.shape = [batch_size, seq_len, dim // 2, 2]
    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)
    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)
    
    # 转为复数域
    xq_ = torch.view_as_complex(xq_)
    xk_ = torch.view_as_complex(xk_)
    
    # 应用旋转操作，然后将结果转回实数域
    # xq_out.shape = [batch_size, seq_len, dim]
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.wq = Linear(...)
        self.wk = Linear(...)
        self.wv = Linear(...)
        
        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)

    def forward(self, x: torch.Tensor):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(batch_size, seq_len, dim)
        xk = xk.view(batch_size, seq_len, dim)
        xv = xv.view(batch_size, seq_len, dim)

        # attention 操作之前，应用旋转位置编码
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # scores.shape = (bs, seqlen, seqlen)
        scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim)
        scores = F.softmax(scores.float(), dim=-1)
        output = torch.matmul(scores, xv)  # (batch_size, seq_len, dim)
  # ......
```

# 强化学习

<img src="assets\image-20250423174556408.png" alt="image-20250423174556408" style="zoom:67%;" /> 

<img src="assets\image-20250423174611669.png" alt="image-20250423174611669" style="zoom:50%;" /> 

<img src="assets\image-20250423174754609.png" alt="image-20250423174754609" style="zoom:67%;" /> 

## DQN

## PPO

## DPO

**Direct Preference Optimization，直接偏好优化**

## RLHF

**Reinforcement Learning from Human Feedback，人类反馈强化学习**

<img src="assets/image-20250701205352001.png" alt="image-20250701205352001" style="zoom:50%;" /> 

 

# 计算机视觉(CV)

## 超分辨率Super-Resolution

<img src="assets\image-20250403230441211.png" alt="image-20250403230441211" style="zoom:80%;" /> 

### 插值

<img src="assets\image-20250403223119126.png" alt="image-20250403223119126" style="zoom:67%;" /> 

<img src="assets\image-20250403221621550.png" alt="image-20250403221621550" style="zoom: 67%;" /> 

<img src="assets\image-20250403221651797.png" alt="image-20250403221651797" style="zoom:80%;" /> 

<img src="assets\image-20250403222350685.png" alt="image-20250403222350685" style="zoom:67%;" /> 

<img src="assets\image-20250403222407214.png" alt="image-20250403222407214" style="zoom:67%;" /> 

<img src="assets\image-20250403222500027.png" alt="image-20250403222500027" style="zoom:67%;" /> 

<img src="assets\image-20250403222851881.png" alt="image-20250403222851881" style="zoom:67%;" />  

<img src="assets\image-20250403230104523.png" alt="image-20250403230104523" style="zoom:67%;" /> 

<img src="assets\image-20250403230120013.png" alt="image-20250403230120013" style="zoom:80%;" />  

<img src="assets\image-20250403223101220.png" alt="image-20250403223101220" style="zoom:67%;" /> 

<img src="assets\image-20250403225647531.png" alt="image-20250403225647531" style="zoom:67%;" /> 

## 图像分割

### 语义分割

### 实例分割

### SAM

[[2304.02643] Segment Anything](https://arxiv.org/abs/2304.02643)

<img src="assets/image-20250706150623193.png" alt="image-20250706150623193" style="zoom:80%;" /> 

<img src="assets/image-20250706152441484.png" alt="image-20250706152441484" style="zoom:80%;" /> 

### SEEM 

[[2304.06718] Segment Everything Everywhere All at Once](https://arxiv.org/abs/2304.06718)

<img src="assets/image-20250706153823730.png" alt="image-20250706153823730" style="zoom:80%;" /> 

<img src="assets/image-20250706153916422.png" alt="image-20250706153916422" style="zoom:80%;" /> 

  

## 目标检测

### yolo

### SSD

### faster RCNN

### DINO

[[2203.03605] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection](https://arxiv.org/abs/2203.03605)



<img src="assets/image-20250706181139516.png" alt="image-20250706181139516" style="zoom:80%;" /> 

 

# 自然语言处理(NLP)

## TF-IDF

<img src="assets\image-20250517175934886.png" alt="image-20250517175934886" style="zoom:80%;" /> 

<img src="assets\image-20250517180016963.png" alt="image-20250517180016963" style="zoom:67%;" /> 

 <img src="assets\image-20250517180244359.png" alt="image-20250517180244359" style="zoom:67%;" /> 

<img src="assets\image-20250517182015899.png" alt="image-20250517182015899" style="zoom:67%;" /> 

 <img src="assets\image-20250517182311893.png" alt="image-20250517182311893" style="zoom:67%;" /> 

TF−IWF 这种加权方法 **降低了文档集/语料库中同类文本对词权重的影响，更加精确地表达了词在待查文档中的重要程度**。

传统 TF−IDF 所求的权值一般很小，甚至接近于 0，精确度也不高，而 TF−IWF 的计算结果恰能解决权值过小的问题。

## LDA 主题模型

<img src="assets\image-20250517222951453.png" alt="image-20250517222951453" style="zoom: 80%;" /> 

<img src="assets\image-20250518021410497.png" alt="image-20250518021410497" style="zoom:80%;" /> 

## Tokenizer

 <img src="assets\image-20250519001025901.png" alt="image-20250519001025901" style="zoom:67%;" /> 

<img src="assets\image-20250519001402351.png" alt="image-20250519001402351" style="zoom: 67%;" /> 

<img src="assets\image-20250519001448704.png" alt="image-20250519001448704" style="zoom:67%;" /> 

<img src="assets\image-20250519001841783.png" alt="image-20250519001841783" style="zoom:67%;" /> 

<img src="assets\image-20250519005132956.png" alt="image-20250519005132956" style="zoom:67%;" /> 

 <img src="assets\image-20250519005146975.png" alt="image-20250519005146975" style="zoom: 80%;" /> 

<img src="assets\image-20250519003305498.png" alt="image-20250519003305498" style="zoom:67%;" /> 

### BPE

<img src="assets\image-20250519002401003.png" alt="image-20250519002401003" style="zoom:67%;" /> 

<img src="assets\image-20250519005314906.png" alt="image-20250519005314906" style="zoom:80%;" />  

<img src="assets\image-20250519004346279.png" alt="image-20250519004346279" style="zoom: 80%;" /> 

<img src="assets\image-20250519004607890.png" alt="image-20250519004607890" style="zoom:80%;" /> 

### WordPiece

<img src="assets\image-20250519005951093.png" alt="image-20250519005951093" style="zoom:80%;" /> 

### Unigram

<img src="assets\image-20250519010829154.png" alt="image-20250519010829154" style="zoom:80%;" /> 

<img src="assets\image-20250519010742662.png" alt="image-20250519010742662" style="zoom:80%;" /> 

<img src="assets\image-20250519011215732.png" alt="image-20250519011215732" style="zoom:80%;" /> 

# 分布式训练

## 数据并行(DP/DDP)

<img src="assets\image-20250430134619762.png" alt="image-20250430134619762" style="zoom:67%;" /> 

<img src="assets\image-20250430135137120.png" alt="image-20250430135137120" style="zoom:67%;" /> 

<img src="assets\image-20250430180050311.png" alt="image-20250430180050311" style="zoom:67%;" /> 

<img src="assets\image-20250430180835259.png" alt="image-20250430180835259" style="zoom:80%;" /> 

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms

# 定义模型（示例用简单的线性模型）
class MyModel(torch.nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = torch.nn.Linear(10, 10)
    def forward(self, x):
        return self.fc(x)

# 训练函数
def train(rank, world_size):
    # 初始化进程组
    # 常见的通信后端包括 gloo 和 nccl，其中 nccl 是在 GPU 上最常用的后端
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)

    # 设置当前 GPU
    torch.cuda.set_device(rank)  # 若是在多机上，此处设置为local_rank

    # 创建模型并转移到当前 GPU
    model = MyModel().cuda(rank)  # 若是在多机上，此处设置为local_rank

    # 将模型包装为 DDP 模型
    model = DDP(model, device_ids=[rank])	# 若是在多机上，此处设置为local_rank

    # 定义损失函数和优化器
    criterion = torch.nn.CrossEntropyLoss().cuda(rank)	# 若是在多机上，此处设置为local_rank
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    # 加载数据集（示例使用 CIFAR10）
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

    # 使用 DistributedSampler 确保每个 GPU 得到不同的数据
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)	# 若是在多机上，此处设置为local_rank
    train_loader = DataLoader(dataset=train_dataset, batch_size=32, sampler=train_sampler)

    # 开始训练
    for epoch in range(10):
        model.train()
        train_sampler.set_epoch(epoch)  # 每个 epoch 设置随机种子以保证不同进程之间的数据一致
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.cuda(rank), target.cuda(rank)	# 若是在多机上，此处设置为local_rank
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # 销毁进程组
    dist.destroy_process_group()

# 主函数
def main():
    world_size = 2  # 使用 2 张 GPU

    # 使用 multiprocessing 启动多进程，每个 GPU 对应一个进程
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main()
```

<img src="assets\image-20250430180610671.png" alt="image-20250430180610671" style="zoom: 80%;" /> 

<img src="assets\image-20250430180641620.png" alt="image-20250430180641620" style="zoom:80%;" /> 

## 流水线并行(PP)

**Pipeline Parallelism**

### GPipe

https://arxiv.org/pdf/1811.06965

<img src="assets/image-20250801175312885.png" alt="image-20250801175312885" style="zoom:80%;" />

## 张量并行(TP)



## DeepSpeed

# 推荐系统

<img src="assets\image-20250517192945429.png" alt="image-20250517192945429" style="zoom:67%;" /> 

<img src="assets\image-20250517193115047.png" alt="image-20250517193115047" style="zoom: 80%;" /> 

<img src="assets\image-20250517193147276.png" alt="image-20250517193147276" style="zoom: 80%;" /> 

## 召回

<img src="assets\image-20250517193240590.png" alt="image-20250517193240590" style="zoom:80%;" /> 

# 模型轻量化

## 蒸馏

<img src="assets\image-20250423153200155.png" alt="image-20250423153200155" style="zoom:67%;" /> 

<img src="assets\image-20250423153133783.png" alt="image-20250423153133783" style="zoom:80%;" /> 

<img src="assets\image-20250423153745152.png" alt="image-20250423153745152" style="zoom:67%;" /> 

<img src="assets\image-20250423153759589.png" alt="image-20250423153759589" style="zoom:67%;" /> 

<img src="assets\image-20250423153301493.png" alt="image-20250423153301493" style="zoom: 67%;" /> 

<img src="assets\image-20250423153634116.png" alt="image-20250423153634116" style="zoom: 67%;" /> 

## 量化

<img src="assets\image-20250512221958977.png" alt="image-20250512221958977" style="zoom: 50%;" /> 

<img src="assets\image-20250512223845596.png" alt="image-20250512223845596" style="zoom:80%;" /> 

<img src="assets\image-20250512223751019.png" alt="image-20250512223751019" style="zoom: 67%;" /> 

<img src="assets\image-20250512223812038.png" alt="image-20250512223812038" style="zoom:67%;" /> 

<img src="assets\image-20250512223822901.png" alt="image-20250512223822901" style="zoom: 67%;" /> 

###  训练后量化(PTQ)

**Post-training-quantization**

<img src="assets\image-20250512224531232.png" alt="image-20250512224531232" style="zoom: 67%;" /> 

<img src="assets\image-20250512224557276.png" alt="image-20250512224557276" style="zoom: 67%;" /> 

### 量化感知训练(QAT)

**Quantization-aware-training**

<img src="assets\image-20250514012604076.png" alt="image-20250514012604076" style="zoom:67%;" /> 

## 低秩分解

# 大语言模型

## 微调

### 低秩适应(LoRA)

**Low-Rank Adaptation**

<img src="assets\image-20250514020113800.png" alt="image-20250514020113800" style="zoom:80%;" /> 

<img src="assets\image-20250514021140044.png" alt="image-20250514021140044" style="zoom: 67%;" /> 

<img src="assets\image-20250514021009857.png" alt="image-20250514021009857" style="zoom:67%;" /> 

### 监督微调（SFT）

**Supervised Fine-Tuning**



## Tool Learning

<img src="assets\image-20250515103539089.png" alt="image-20250515103539089" style="zoom:67%;" /> 

<img src="assets\image-20250515103951754.png" alt="image-20250515103951754" style="zoom: 67%;" /> 

<img src="assets\image-20250515104113191.png" alt="image-20250515104113191" style="zoom: 67%;" /> 

<img src="assets\image-20250515102328271.png" alt="image-20250515102328271" style="zoom:67%;" /> 

<img src="assets\image-20250515104237189.png" alt="image-20250515104237189" style="zoom:67%;" /> 

 <img src="assets\image-20250515104344725.png" alt="image-20250515104344725" style="zoom:67%;" /> 



## 多模态

<img src="assets\image-20250407132547580.png" alt="image-20250407132547580" style="zoom: 67%;" />  

### 对比学习

<img src="assets\image-20250407125751136.png" alt="image-20250407125751136" style="zoom:67%;" /> 

<img src="assets\image-20250407125808900.png" alt="image-20250407125808900" style="zoom:80%;" /> 

<img src="assets\image-20250407125834294.png" alt="image-20250407125834294" style="zoom:67%;" /> 

<img src="assets\image-20250407130218548.png" alt="image-20250407130218548" style="zoom:67%;" /> 

<img src="assets\image-20250407130709955.png" alt="image-20250407130709955"  /> <img src="assets\image-20250407130721905.png" alt="image-20250407130721905" style="zoom:67%;" />  

<img src="assets\image-20250407130751914.png" alt="image-20250407130751914" style="zoom: 67%;" /> 

<img src="assets\image-20250407130853721.png" alt="image-20250407130853721" style="zoom:67%;" /> 

#### CLIP

![image-20250407132319259](assets\image-20250407132319259.png) 

<img src="assets\image-20250407121825805.png" alt="image-20250407121825805" style="zoom:67%;" /> 

<img src="assets\image-20250407121854146.png" alt="image-20250407121854146" style="zoom:67%;" /> 

![image-20250407121957171](assets\image-20250407121957171.png) 

**预训练**

<img src="assets\image-20250407122027474.png" alt="image-20250407122027474" style="zoom:67%;" /> 

<img src="assets\image-20250407122045422.png" alt="image-20250407122045422" style="zoom:67%;" /> 

<img src="assets\image-20250407122115156.png" alt="image-20250407122115156" style="zoom:80%;" />

**推理**

![image-20250407122205488](assets\image-20250407122205488.png)

<img src="assets\image-20250407122422622.png" alt="image-20250407122422622" style="zoom:80%;" />

<img src="assets\image-20250407123811712.png" alt="image-20250407123811712" style="zoom:67%;" /> 

#### CLAP

<img src="assets\image-20250502174823331.png" alt="image-20250502174823331" style="zoom:80%;" /> 

#### BLIP

### DINO

[[2104.14294] Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

<img src="assets/image-20250706183029340.png" alt="image-20250706183029340" style="zoom:80%;" /> 

### VLM

### BLIP2

[[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)

<img src="assets/image-20250708005958218.png" alt="image-20250708005958218" style="zoom:80%;" /> 

<img src="assets/image-20250708010034534.png" alt="image-20250708010034534" style="zoom:80%;" /> 

## MoE 架构

[A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

<img src="assets\image-20250505183854419.png" alt="image-20250505183854419" style="zoom: 50%;" /> <img src="assets\image-20250505183955344.png" alt="image-20250505183955344" style="zoom:50%;" />   

<img src="assets\image-20250505182240868.png" alt="image-20250505182240868" style="zoom:67%;" /> 

<img src="assets\image-20250505182248298.png" alt="image-20250505182248298" style="zoom:67%;" /> 

<img src="assets\image-20250505182657503.png" alt="image-20250505182657503" style="zoom:67%;" /> 

<img src="assets\image-20250505182959308.png" alt="image-20250505182959308" style="zoom: 67%;" /> 

<img src="assets\image-20250505182947162.png" alt="image-20250505182947162" style="zoom:67%;" /> 

路由器（或门控网络）本身也是一个 FFN，它根据特定的输入选择专家。路由器会输出概率值，并利用这些概率来选择最匹配的专家：

**专家层返回被选定专家的输出，并乘以门控值（选择概率）。**

路由器和专家（其中仅选择少部分）共同构成了 MoE 层。

给定的 MoE 层有两种类型：稀疏专家混合模型（Sparse Mixture of Experts）和密集专家混合模型（Dense Mixture of Experts）。

两者都使用路由器来选择专家，但稀疏 MoE 只选择少数几个专家，而密集 MoE 则选择全部专家，但可能会以不同的分布进行选择。

<img src="assets\image-20250505184157768.png" alt="image-20250505184157768" style="zoom: 67%;" /> 

### 负载均衡

<img src="assets\image-20250505192124109.png" alt="image-20250505192124109" style="zoom:50%;" /> 

然而，这个简单的功能往往会导致路由器总是选择相同的专家，因为某些专家可能比其他专家学习得更快。

这不仅会导致专家选择的不均匀分布，还会导致某些专家几乎没有被训练过。这会在训练和推理过程中引发问题。

因此，我们希望在训练和推理期间，各个专家的使用具有同等的重要性，这就是所谓的负载平衡。某种程度上，这是为了防止模型在同一组专家上过拟合。

#### KeepTopK

一种对路由器进行负载平衡的方法是使用一个简单的扩展策略，称为 KeepTopK。通过引入可训练的（高斯）噪声，我们可以防止总是选择相同的专家：

<img src="assets\image-20250505185544253.png" alt="image-20250505185544253" style="zoom: 67%;" /> 

然后，除希望激活的前 k 个专家（例如 2 个）以外的所有专家权重都将被设为 $-\infty$。

将这些权重设为 $-\infty$ 时，SoftMax 操作后的输出概率将变为 0。

<img src="assets\image-20250505194542278.png" alt="image-20250505194542278" style="zoom: 67%;" /> 



#### 辅助损失

#### 专家容量

### Switch Transformer

### GShard

### Vision-MoE

### 优势

- **提高模型性能**：通过将多个专家的预测结果进行整合，MoE 模型可以在不同的数据子集或任务方面发挥每个专家的优势，从而提高整体模型的性能。例如，在图像分类任务中，一个专家可能擅长识别动物图片，而另一个专家可能擅长识别车辆图片，通过门控网络的合理分配，MoE 模型可以更准确地对不同类型的图片进行分类。
- **减少计算成本**：与传统的密集模型相比，MoE 模型在处理每个输入样本时，只有相关的专家会被激活，而不是整个模型的所有参数都被使用。这意味着 MoE 模型可以在保持较高性能的同时，显著减少计算资源的消耗，特别是在模型规模较大时，这种优势更为明显。例如，对于一个具有数十亿参数的大型语言模型，采用 MoE 架构可以在不增加太多计算成本的情况下，通过增加专家的数量来进一步提升模型的性能。
- **增强模型的可扩展性**：MoE 模型的架构设计使得它可以很容易地扩展到更多的专家和更大的模型规模。通过增加专家的数量，模型可以覆盖更广泛的数据特征和任务类型，从而在不增加计算复杂度的情况下，提升模型的表达能力和泛化能力。这种可扩展性为处理大规模、复杂的数据集提供了有效的解决方案，例如在处理多模态数据（包含文本、图像、语音等多种类型的数据）时，MoE 模型可以通过设置不同的专家来专门处理不同模态的数据，实现更高效的多模态融合。

### 挑战

- 计算成本与资源管理
  - **内存需求高**：MoE模型需要将所有专家的参数都加载到内存中，即使在推理过程中只使用其中一部分专家。例如，以Mixtral 8x7B这样的MoE模型为例，需要有足够的VRAM来容纳一个47B参数的稠密模型。这是因为MoE模型中只有FFN层被视为独立的专家，而模型的其他参数是共享的。高内存需求使得在资源有限的情况下部署和运行MoE模型变得困难，特别是在需要处理大规模参数模型时，对硬件设备的要求更为苛刻。
  - **分布式训练复杂**：为了应对大规模模型的训练，通常需要采用分布式训练的方式。但在MoE模型中，由于专家之间的数据交换和并行训练需要机间all-to-all通信来实现，这增加了通信成本。随着模型规模的增大，通信开销也相应增加，可能导致训练效率降低。例如，在一个大规模分布式训练场景中，若模型参数规模达到数十亿甚至更大，通信延迟和网络拥塞问题可能会严重影响训练速度和性能。因此，在实际部署过程中，需要仔细设计通信策略和优化网络拓扑，以降低通信延迟和潜在的网络拥塞。
  - **专家容量限制**：为了防止特定专家过载并确保工作负载平衡，通常会对每个专家可以同时处理的输入数量设置阈值。例如，采用top-2路由和1.25的容量因子，这意味着每个输入选择两个专家，每个专家处理其通常容量的1.25倍。这种策略虽然可以在一定程度上平衡负载，但也可能导致部分数据无法及时处理或需要重新分配，影响训练和推理的效率。此外，专家容量的设置需要根据具体的任务和模型规模进行调整，这增加了模型配置和管理的复杂性。
- 过拟合与泛化问题
  - **过拟合风险**：与稠密模型相比，MoE模型在微调时更易产生过拟合现象。这是因为MoE模型的参数量虽然大，但在实际应用中只激活部分专家，模型的复杂度相对较高。例如，拥有1.6T参数量的MoE预训练模型Switch Transformer，在SuperGLUE等常见基准上进行微调时，其整体性能却落后于较小的模型。这表明在微调过程中，模型可能会过度拟合训练数据中的噪声和细节，而无法很好地泛化到新的、未见过的数据上。
  - **泛化能力不足**：MoE模型的泛化能力在某些任务上表现不佳，尤其是在需要对输入数据进行深入理解和推理的任务中。例如，在重理解任务（如SuperGLUE）上，MoE模型的表现不如对应的稠密模型。这可能是因为MoE模型在训练过程中，专家之间的协作和知识共享不够充分，导致模型对特定任务的理解和处理能力有限。此外，门控网络的设计和训练也可能影响模型的泛化能力，如果门控网络不能准确地将输入数据分配给最合适的专家，就会影响模型的整体性能。
  - **微调策略选择**：为了提高MoE模型在微调阶段的泛化能力，需要选择合适的微调策略。一种可行的方法是尝试冻结所有非专家层的权重，只对MoE层的参数进行更新。实验结果显示，这种方法几乎与更新所有参数的效果相当，同时可以加速微调过程并降低显存需求。此外，使用较小的批量大小和较高的学习率进行微调，也有助于提高模型的泛化性能。然而，这些策略的选择需要根据具体的任务和模型情况进行调整，不同的任务可能需要不同的微调策略来达到最佳效果。

## 开源大模型

### 千问系列

#### Qwen 2.5

对于稠密模型，团队延续了 Qwen2 的 Transformer 解码器架构，并在此基础上进行了优化。该架构包括以下关键组件：

- **分组查询注意力（Grouped Query Attention, GQA）**：用于高效地利用 KV 缓存。
- **SwiGLU 激活函数**：增强非线性激活。
- **旋转位置编码（RoPE）**：用于编码位置信息。
- **QKV 偏置**：用于提升注意力机制的表现。
- **RMSNorm**：在预归一化后使用，以保证训练过程稳定。
- **BBPE（Bytewise Byte Pair Encoding）**：用于分词。

#### Qwen3

<img src="assets/image-20250704205903234.png" alt="image-20250704205903234" style="zoom:80%;" /> 

#### Qwen 2 vl

https://arxiv.org/abs/2409.12191

1. **读懂不同分辨率和不同长宽比的图片**：Qwen2-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中取得了全球领先的表现。
2. **理解20分钟以上的长视频**：Qwen2-VL 可理解长视频，并将其用于基于视频的问答、对话和内容创作等应用中。
3. **能够操作手机和机器人的视觉智能体**：借助复杂推理和决策的能力，Qwen2-VL 可集成到手机、机器人等设备，根据视觉环境和文字指令进行自动操作。
4. **多语言支持**：为了服务全球用户，除英语和中文外，Qwen2-VL 现在还支持理解图像中的多语言文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。

模型架构：整体上我们仍然延续了 Qwen-VL 中 ViT 加 Qwen2 的串联结构，在三个不同尺度的模型上，我们都采用 600M 规模大小的 ViT，并且支持图像和视频统一输入。为了让模型更清楚地感知视觉信息和理解视频，我们还进行了以下升级：

- 实现了对**原生动态分辨率**的全面支持。与上一代模型相比，Qwen2-VL 能够处理任意分辨率的图像输入，不同大小图片被转换为动态数量的 tokens，最小只占 4 个 tokens。

  这种设计不仅确保了模型输入与图像原始信息之间的高度一致性，更是模拟了人类视觉感知的自然方式，赋予模型处理任意尺寸图像的强大能力，使其在图像处理领域展现出更加灵活和高效的表现。

- **多模态旋转位置嵌入（M-ROPE）**。传统的旋转位置嵌入只能捕捉一维序列的位置信息，而 M-ROPE 通过将原始旋转嵌入分解为代表时间、高度和宽度的三个部分，使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息。

  这一创新赋予了语言模型强大的多模态处理和推理能力，能够更好地理解和建模复杂的多模态数据。

<img src="assets/image-20250716160806924.png" alt="image-20250716160806924" style="zoom: 50%;" /> 



#### Qwen 2.5 vl

https://arxiv.org/abs/2502.13923

<img src="assets/image-20250716115339642.png" alt="image-20250716115339642" style="zoom:50%;" /> 



### GPT系列

### DeepSeek系列

#### DeepSeek-v3

[[2412.19437] DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)

<img src="assets/image-20250727185032313.png" alt="image-20250727185032313" style="zoom: 80%;" /> 

#### DeekSeep-R1

[[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

- 我们直接将RL应用于基础模型，而不依赖监督微调 (SFT) 作为初步步骤。这种方法允许模型探索解决复杂问题的思维链 (CoT)，从而得到DeepSeek-R1-Zero。DeepSeek-R1-Zero展示了自我验证、反思和生成长思维链等能力，标志着研究界的一个重要里程碑。值得注意的是，这是首次公开研究，验证了LLMs的推理能力可以纯粹通过RL来激励，而不需要SFT。

### LLaMA系列

#### LLaMA

[[2302.13971] LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- 只使用了公开数据集进行训练，完全可复现、可分享。
- 预归一化：使用 **RMSNorm**对 Transformer 层的输入进行归一化。
- **SwiGLU **激活函数。
- **RoPE** 位置编码。
- **AdamW** 优化器，**cosine 学习率调度**。

#### LLaMA-3

[The Llama 3 Herd of Models | Research - AI at Meta](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)

<img src="assets/image-20250727193954139.png" alt="image-20250727193954139" style="zoom:80%;" /> 

<img src="assets/image-20250727194138629.png" alt="image-20250727194138629" style="zoom:80%;" />  

###   LLaVA系列

#### LLaVA

[[2304.08485] Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

<img src="assets/image-20250706155302142.png" alt="image-20250706155302142" style="zoom:80%;" /> 

#### LLaVA-1.5

[[2310.03744] Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)

<img src="assets/image-20250706160511953.png" alt="image-20250706160511953" style="zoom:80%;" /> 

#### LLaVA-OneVision

[[2408.03326] LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)

<img src="assets/image-20250706160640853.png" alt="image-20250706160640853" style="zoom:80%;" /> 

 

## 推理分割（Reasoning Segmentation）

### LISA（CVPR2024）

[[2308.00692] LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/abs/2308.00692)

[(43 封私信 / 80 条消息) 论文阅读笔记：LISA: Reasoning Segmentation via Large Language Model - 知乎](https://zhuanlan.zhihu.com/p/656977939)

<img src="assets/image-20250706154427655.png" alt="image-20250706154427655" style="zoom:80%;" /> 

<img src="assets/image-20250706164643164.png" alt="image-20250706164643164" style="zoom:67%;" /><img src="assets/image-20250706145942268.png" alt="image-20250706145942268" style="zoom:80%;" /> 

#### 贡献点

- 我们介绍了推理分割任务，该任务需要基于隐式人类指令进行推理。这项任务强调了自我推理能力的重要性，这对于构建真正智能的感知系统至关重要。
- 我们建立了一个包含一千多个图像指令对的推理分割 Benchmark：ReasonSeg。对于评估和鼓励社区开发新技术至关重要。
- 我们提出了我们的模型：LISA，它采用嵌入式掩码范例来整合新的分割能力。当在无推理数据集上进行训练时，LISA 在推理分割任务中表现出强大的零样本能力，并通过仅对涉及推理的 239 张图像指令对进行微调而进一步提高了性能。

#### LISA++

<img src="assets/image-20250706163536936.png" alt="image-20250706163536936" style="zoom:80%;" /> 

- 增强分割：添加了实例分割能力，提供了更详细的场景分析以及现有的多区域语义分割。
- 更自然的对话：提高多轮对话的能力，能够将分割结果直接合并到文本响应中，即对话 (SiD) 中的分割。
- 这些改进是通过管理通用分割数据集的现有样本来实现的，旨在专门增强分割和对话技能，而无需结构变化和额外的数据源。 

### SegLLM（ICLR2025）

[[2410.18923] SegLLM: Multi-round Reasoning Segmentation](https://arxiv.org/abs/2410.18923)

<img src="assets/image-20250707153911340.png" alt="image-20250707153911340" style="zoom: 67%;" /> 

### LLM-Seg（CVPR2024）

[[2404.08767] LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning](https://arxiv.org/abs/2404.08767)

<img src="assets/image-20250707234958688.png" alt="image-20250707234958688" style="zoom: 67%;" /> 

<img src="assets/image-20250708005702525.png" alt="image-20250708005702525" style="zoom:80%;" /> 

 

### OpenMaskDINO3D

[[2506.04837] OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model](https://arxiv.org/abs/2506.04837)

<img src="assets/image-20250706180932844.png" alt="image-20250706180932844" style="zoom:80%;" /> 



### MIRAS

[[2502.09447] Pixel-Level Reasoning Segmentation via Multi-turn Conversations](https://arxiv.org/abs/2502.09447)<img src="assets/image-20250706191141337.png" alt="image-20250706191141337" style="zoom:80%;" /> 



### HyperSeg

https://arxiv.org/abs/2411.17606

<img src="assets/image-20250707203130612.png" alt="image-20250707203130612" style="zoom:67%;" /> 

### See say and segment

[[2312.08366] See, Say, and Segment: Teaching LMMs to Overcome False Premises](https://arxiv.org/abs/2312.08366)<img src="assets/image-20250707235510866.png" alt="image-20250707235510866" style="zoom:80%;" /> 
